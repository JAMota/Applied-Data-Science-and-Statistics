---
title: "Class1StatsTopics"
format:
  pdf:
    toc: true
editor: visual
---

## Quarto

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
# install.packages('R2jags', dependencies = TRUE) # installs coda too
# # install.packages('coda', dependencies = TRUE)
# install.packages('lattice',dependencies = TRUE)
# install.packages('MCMCvis', dependencies = TRUE)
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
library(R2jags); library(MCMCvis); library(coda); library(lattice)
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
#creating a sample o n size to fit out posterior distribution
#sample(x, size,...)

```

## Coin example

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# model definition
jags.mod.coin <- function(){
Y ~ dbin(0.5,10) # our data model
P8 <- ifelse(Y>7,1,0) # the probability of interest, aka 8 or higher out of 10
## the ifelse gives the first value if the statement is true and the second value if the statement is false.
## we are not forced to use the binary 1 and 0, we can use any value
}
## jag has 2 syntaxes
## the first one is a stochastic dependence ~
## the second one is a logical dependence <-

##in the jag binom we use the mu parameter and the tau parameter
## the tau parameter is called the precision and is 1/ sigma(σ) ^2

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## Generating 100 samples without discarding one with 1 chain

jags.mod.fit.coin <- jags(data = list(), model.file = jags.mod.coin,
parameters.to.save = c('Y','P8'),n.chains=1,
DIC=FALSE, n.burnin=0,n.iter = 100) ## here we make DICE=False because our code does not contain the likelihood

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

#To get the numerical summary of the above model run we use the print function

print(jags.mod.fit.coin)

## this gives us the mean, standard deviation and some quantiles of the generated samples
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

#We can also plot the simulated samples using the traceplot function

traceplot(jags.mod.fit.coin)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
##For more diagnostics and visualisation tools we can convert the output of the jags function into an MCMC
##object. Using the MCMC object we can look at numerical summaries, traceplots and density plots.

# convert into MCMC object
jagsfit.mcmc.coin <- as.mcmc(jags.mod.fit.coin)
# get numerical summary
summary(jagsfit.mcmc.coin)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
##The functions xyplot and densityplot of the lattice package give us trace plots and density plots of all
##the parameters.

# get traceplots
xyplot(jagsfit.mcmc.coin)
# get density estimate
densityplot(jagsfit.mcmc.coin)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
##If we want to concentrate on individual parameters, we can use the MCMCtrace function of the MCMCvis package

MCMCtrace(jagsfit.mcmc.coin,
params = 'Y', # parameter of interest
type = 'density', # density plot
ind = TRUE, # separate density lines for each chain
pdf = FALSE) # plots are NOT exported into a pdf


MCMCtrace(jagsfit.mcmc.coin,
params = 'P8',
type = 'trace',
ind = TRUE,
pdf = FALSE)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
##We can also extract summaries of interest. For example, we can get the point estimate, and the two endpoints
## of a 95% credible interval using the following

jags.mod.fit.coin$BUGSoutput$summary[,1] # mean
jags.mod.fit.coin$BUGSoutput$summary[,3] # 2.5 percentile
jags.mod.fit.coin$BUGSoutput$summary[,7] # 97.5 percentile

```

## Exercise 1

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

##increasing the number of iterations
## the mean estimate is much closer to the truth, more reliable estimates
jags.mod.fit.coin <- jags(data = list(), model.file = jags.mod.coin,
parameters.to.save = c('Y','P8'),n.chains=1,
DIC=FALSE, n.burnin=0,n.iter = 100000)


print(jags.mod.fit.coin)

## it becomes more accurate as we can see that the mean is closer to the expected real value of 5

```

## Exercise 2

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

##number of chains, how much times does jags runs the simulation independently of each other

jags.mod.fit.coin <- jags(data = list(), model.file = jags.mod.coin,
parameters.to.save = c('Y','P8'),n.chains=3,
DIC=FALSE, n.burnin=0,n.iter = 50)


# convert into MCMC object
jagsfit.mcmc.coin <- as.mcmc(jags.mod.fit.coin)
# get numerical summary
summary(jagsfit.mcmc.coin)

# get trace plots
xyplot(jagsfit.mcmc.coin)
# get density estimate
densityplot(jagsfit.mcmc.coin)

## the outputs are somewhat close for 2 chains but the third one seems to not have converged


```

## Exercise 3

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

#model definition
jags.mod.clinicalTrial <- function(){
Y ~ dbin(0.7,30) # our data model of prob 0.7 and 30 number of trials
P15 <- ifelse(Y<15,1,0) # the probability of interest, aka 15 or less positive responses out of 30
}

jags.mod.fit.clinicalTrial <- jags(data = list(), model.file = jags.mod.clinicalTrial,
parameters.to.save = c('Y','P15'),n.chains=3,
DIC=FALSE, n.burnin=2000,n.iter = 10000) ## here we make DICE=False because our code does not contain the likelihood

print(jags.mod.fit.clinicalTrial)

# the probability of 15 or less trials is on average 0.006

```

## Exercise 4

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

#model definition with var 1 and sd 2
jags.norm3 <- function(){ ## attention to how normal functions parameters are different in JAGS compared to R
Y ~ dnorm(1,1/4) # 1/var=1 1/sd^2= 1/4
X = Y^3
}


jags.mod.fit.norm3 <- jags(data = list(), model.file = jags.norm3,
parameters.to.save = c('X'),n.chains=3,
DIC=FALSE, n.burnin=0,n.iter = 5000)

# jags.mod.fit <- jags(data = list(), model.file = jags.mod.normal3,
# parameters.to.save = c('x'),n.chains=1,
# DIC=FALSE, n.burnin=0,n.iter = 10000)
# print(jags.mod.fit[drop=F]) # look at mean of x


############ all of these bellow are unecessary but there is not harm in checking the plots
# convert into MCMC object
jagsfit.mcmc.norm3 <- as.mcmc(jags.mod.fit.norm3)
# get numerical summary
#summary(jagsfit.mcmc.norm3)

##need to change the entry variable
# get trace plots
xyplot(jagsfit.mcmc.norm3)
# get density estimate
densityplot(jagsfit.mcmc.norm3)


summary(jagsfit.mcmc.norm3)

print(jags.mod.fit.norm3[drop=F])




```

```{r, echo=FALSE, eval=FALSE}

## DORKA solutions for ex 4
jags.mod.normal3 <- function(){
y ~ dnorm(1,1/4) # our data model
x <- y^3
}
jags.mod.fit <- jags(data = list(), model.file = jags.mod.normal3,
parameters.to.save = c('x'),n.chains=1,
DIC=FALSE, n.burnin=0,n.iter = 10000)
print(jags.mod.fit[drop=F]) # look at mean of x
```

## Trace plot note

We need to check both the trace plots and the coefficients to check for
the scale reduction factor, sometimes the factor will be good but we
have no conversion.

## Drug trial exercise without data

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
### Drug example in JAGS
# model
jags.mod.drug <- function(){
theta ~ dbeta(9.2,13.8) # prior for the unknown parameter
y ~ dbin(theta,20) # the data model, since it is a positive or negative we use the binomail for the 20 trials
P.crit <- ifelse(y>=15,1,0) # quantity of interest, we want to know how the probability of at least 15 positives
}


```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
##Even though we don’t have any available data, monitoring the parameters y and P.crit will allow us to
##plot the predictive distribution of y, and get a point estimate for the predictive probability that 15 or more
##patients will experience a positive response.


jags.mod.fit.drug <- jags(data = list(), n.iter = 100000, DIC=FALSE, ## large ammount of iterations
parameters.to.save = c('theta','y','P.crit'),
model.file = jags.mod.drug,n.chains=1)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

## Fying bombs

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## the gamma distribution is good for when we need a vague prior with only positives values on the real line (IR)

##model

jags.mod.bomb <- function(){

##likelihood
for (i in 1:N) {
  y[i] ~ dpois(lambda) ## the likelihood is only to set up what distribution our data follows
}

##prior

lambda ~ dgamma(0.001,0.001)

} ## ends the model definition

y <- c(rep(0,229),rep(1,211),rep(2,93),rep(3,35),rep(4,7),rep(7,1)) ## definite our dataset/occurrences
N <- length(y)
bomb.data <- list('N','y')

# initial values (for 2 chains)

## since the values we got from the occurrences we can see that the values are very low
bomb.inits1 <- list('lambda'=0.8)
bomb.inits2 <- list('lambda'=1)
bomb.inits <- list(bomb.inits1,bomb.inits2)

# parameters to monitor
jags.param <- c('lambda')

# model fit
jags.mod.fit.bomb <- jags(data = bomb.data, inits = bomb.inits,
  parameters.to.save = jags.param, n.chains = 2,
  n.iter = 10000, model.file = jags.mod.bomb) ## if we don't set our burning and thinning parameters JAGS will chose these for us automatically and they may not always be the best values


# look at numerical summary
print(jags.mod.fit.bomb)
# point estimate for the parameter is 0.933                   
                   
```

```{r}

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```
