---
title: "AdvancedStatsAssignment"
format:
  pdf:
    toc: true
editor: visual
---

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, output=FALSE}

library(C50)
library(caret) # function for creating train/test datasets, and confusion matrices
library(class) # this has the knn function
library(ggplot2)
library(tidyverse)
library(tidyr)
library(DataExplorer)
library(caTools)
library(kernlab)
library(e1071)

library(randomForest)
library(mlbench)


library(RColorBrewer)
library(sf)
library(rgdal)

#install.packages("sf")

library(reshape2)

library(R2jags); library(MCMCvis); library(coda); library(lattice)

classificationDF = read_csv("Classification.csv")

ScotlandDF = read_csv("Scotland.csv")

surgicalDF = read_csv("surgical.csv")

## https://www.r-bloggers.com/2012/04/getting-started-with-jags-rjags-and-bayesian-modelling/

```

## Question 1

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE}

source("ScotlandMap.R") # need to read in the Scotland Map function
testdat = runif(56) # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland random numbers")

```

### Question 1 a)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

ScotlandDF$SMR = ScotlandDF$Observed /ScotlandDF$Expected

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), warning=FALSE}

hist(ScotlandDF$SMR, breaks = 20, col = "lightblue",
     main = "Distribution of Standard Mortality Ratios",
     xlab = "SMR")

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat = ScotlandDF$SMR # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland SMR by region")


```

### Question 1 b)

##fix this latex TODO
<!-- For the model: \begin{align*} -->
<!-- Obs_i \sim \text{Pois}(\mu_i) \ -->
<!-- \log(\mu_i) = \log(\text{Exp}_i) + \beta_0 + \theta_i \ -->
<!-- \text{RR}_i = \exp(\beta_0) \exp(\theta_i) -->
<!-- \end{align*}= \log(\text{Exp}\_i) + \beta\_0 + \theta\_i\ -->
<!-- \text{RR}\_i = \exp(\beta\_0) \exp(\theta\_i) \\end{align\*}= -->
<!-- \log(\text{Exp}\_i) + \beta\_0 + \theta\_i\ -->
<!-- \text{RR}\_i = \exp(\beta\_0) \exp(\theta\_i) \\end{align\*} -->

$\beta0$ is the intercept that represents the baseline for when all the
other variables are 0 which can also be interpreted as the log number
expected of cases per administrative area.

$\theta i$ is the effect that captures the variation in the observed
number of cases that cannot be explained by the expected numbers alone.
$\theta i$ also represents the deviation of the log observed number of
cases from the log expected number of cases for area i.

In the Poisson model, $\theta i$ represents the deviation of the
observed number of cases in area i from the expected number of cases in
area i, given the reference rates. In other words, it captures how much
higher or lower the observed number of cases is compared to what we
would expect based on the reference rates

The role of Î¸i in the estimation of the relative risk is to allow for
variation in the risk of disease across different areas, over and above
what is explained by the reference rates. By including a random effect
for each area in the model, we are able to estimate the relative risk
for each area, while accounting for the fact that some areas may have
higher or lower risk due to factors that are not captured by the
reference rates alone.

### Question 1 c)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jags.mod.lipCancer = function(){
  # Priors
  beta0 ~ dnorm(0, 1/1000)
  tau ~ dgamma(1, 0.05)
  
  # Likelihood
  for (i in 1:N) {
    obs[i] ~ dpois(mu[i])
    log(mu[i]) = log(expo[i]) + beta0 + theta[i]
    theta[i] ~ dnorm(0, tau)
    RR[i] = exp(beta0) * exp(theta[i])
  }
} 

# Parameters to monitor
jags.param.lipCancer = c("beta0", "theta", "tau", "RR")

# Data
jags.data.lipCancer <- list(N = nrow(ScotlandDF),
     obs = ScotlandDF$Observed,
     expo = ScotlandDF$Expected)

## initial values, have to be for the stochaic for thetta it will be a repetition because of the i
inits1 <- list('tau' = 100, 'sigma' = 20, 'theta'=c(10,-5,-25))
inits2 <- list('tau'=100000,'sigma'=-100,'theta'=c(-100,100,500))

jags.inits.lipCancer = list(inits1,inits2)

## fit the jags model

jags.mod.fit.lipCancer <- jags(data = jags.data.lipCancer, #inits = jags.inits.lipCancer,
parameters.to.save = jags.param.lipCancer, n.chains = 2, n.iter = 15000,
n.burnin = 7500,n.thin=1,model.file = jags.mod.lipCancer, DIC=FALSE)



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jagsfitlipCancer.mcmc <- as.mcmc(jags.mod.fit.lipCancer )
MCMCtrace(jagsfitlipCancer.mcmc ,type = 'trace',ind = TRUE, pdf = FALSE)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

gelman.diag(jagsfitlipCancer.mcmc)


```

### question 1.d)

we know have to monitor RR as well 

<!-- # extract posterior mean for mu -->
<!-- pos <- substr(rownames(jags.mod.fit.lipCancer$BUGSoutput$summary),1,2)=='mu' -->

<!-- muRegions = jags.mod.fit.lipCancer$BUGSoutput$summary[pos,1] -->

<!-- ScotlandDF$mu = muRegions -->

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

##extract posterior RR
posRR <- substr(rownames(jags.mod.fit.lipCancer$BUGSoutput$summary),1,2)=='RR'

RRRegions = jags.mod.fit.lipCancer$BUGSoutput$summary[posRR,1]

ScotlandDF$RR = RRRegions

```

Checking the difference between the average RR and the SMR for each region 

should I do RR/SMR and see how close it is to 1?

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

ScotlandDF$SMR / ScotlandDF$RR

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat = ScotlandDF$RR # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland SMR by region")


```


### Question 1.e)

Now I will have to check when RR is > 1 using similarly the summary Confidicence intervals

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jags.mod.lipCancer = function(){
  # Priors
  beta0 ~ dnorm(0, 1/1000)
  tau ~ dgamma(1, 0.05)
  
  # Likelihood
  for (i in 1:N) {
    obs[i] ~ dpois(mu[i])
    log(mu[i]) = log(expo[i]) + beta0 + theta[i]
    theta[i] ~ dnorm(0, tau)
    RR[i] = exp(beta0) * exp(theta[i])
  }
  
  PRRAbove = ifelse(RR>1,1,0)
  
} 

# Parameters to monitor
jags.param.lipCancer = c("beta0", "theta", "tau", "RR", "PRRAbove")

# Data
jags.data.lipCancer <- list(N = nrow(ScotlandDF),
     obs = ScotlandDF$Observed,
     expo = ScotlandDF$Expected)

## initial values, have to be for the stochaic for thetta it will be a repetition because of the i
inits1 <- list('tau' = 100, 'sigma' = 20, 'theta'=c(10,-5,-25))
inits2 <- list('tau'=100000,'sigma'=-100,'theta'=c(-100,100,500))

jags.inits.lipCancer = list(inits1,inits2)

## fit the jags model

jags.mod.fit.lipCancer <- jags(data = jags.data.lipCancer, #inits = jags.inits.lipCancer,
parameters.to.save = jags.param.lipCancer, n.chains = 2, n.iter = 15000,
n.burnin = 7500,n.thin=1,model.file = jags.mod.lipCancer, DIC=FALSE)



```



```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}


# ##extract posterior RR
# posProbRR <- substr(rownames(jags.mod.fit.lipCancer$BUGSoutput$summary),1,6)=='ProbRR'
# 
# jags.mod.fit.lipCancer$BUGSoutput$summary[posProbRR,1] ##mean
# jags.mod.fit.lipCancer$BUGSoutput$summary[posProbRR,3] # 2.5 percentile
# jags.mod.fit.lipCancer$BUGSoutput$summary[posProbRR,7] # 97.5 percentile

## to see which regions RR are in the 95% C.I we will have to see if the 2.5% is small or equal to 1 and if 97.5% is equal or greater than 1

##extract posterior RR
posProbRR <- substr(rownames(jags.mod.fit.lipCancer$BUGSoutput$summary),1,2)=='RR'

#jags.mod.fit.lipCancer$BUGSoutput$summary[posProbRR,1] ##mean
lowerCI = jags.mod.fit.lipCancer$BUGSoutput$summary[posProbRR,3] # 2.5 percentile
upperCI = jags.mod.fit.lipCancer$BUGSoutput$summary[posProbRR,7] # 97.5 percentile


```

Since we know that the 97,5 percentile is always bigger than the the 2,5 percentile to check if the 95% CI of RR is lower than 1, meaning there is no excessive risk,  all we have to do is check if a region 97,5% percentile is smaller than 1

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

ScotlandDF$AboveRR = upperCI
ScotlandDF$isAboveRR = 0

for(i in 1:nrow(ScotlandDF) ){

  if(ScotlandDF$AboveRR[i] > 1){
    ScotlandDF$isAboveRR[i] = 1
  }
  
  
  if(upperCI[i] < 1){
    print(ScotlandDF$Name[i])
  }
}




```


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat = ScotlandDF$isAboveRR # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland SMR by region")


```

And as so here are the regions without any excessive lip cancer.

Map the scotland for the RR instead of SMR

### Question 1f)

#exponential? normal prior?
# for beta0 normal?

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

## Question 2 a)

<!-- To main steps of fitting a Bayesian model using R2jags are the -->
<!-- following. 1. Define the JAGS model (as a function). 2. Prepare the -->
<!-- data, which should be passed on as a list. Note that this list should -->
<!-- include other known variables not in the data, like the length N of the -->
<!-- data vector, when this N is used in the JAGS model definition (e.g. in a -->
<!-- for loop). 3. Identify the parameters whose posterior distributions are -->
<!-- of interest. Here a vector of the parameter names is needed. 4. Define -->
<!-- starting values for JAGS. We need starting values for all the stochastic -->
<!-- nodes (unknown parameters, missing data) in each chain. Initial values -->
<!-- should be passed on as a list. Note that JAGS runs without user-defined -->
<!-- starting values, in which case it generates its own initial values. But -->
<!-- this is not recommended, especially for complex models. 5. Fit the model -->
<!-- in JAGS. The most important arguments of the jags function includes: -->
<!-- data (data list), inits (list of initial values), parameters.to.save -->
<!-- (parameters of interest), n.chains (number of MC chains - each gives a -->
<!-- sequence of simulated values), n.iter (number of iterations in the -->
<!-- sampling process), n.burnin (number of initial samples to discard), -->
<!-- model.file (the JAGS model definition). 6. Convergence diagnostics. -->
<!-- Update if convergence hasn't reached. (See later). 7. Once we are -->
<!-- satisfied that the chains have converged, we can start the statistical -->
<!-- inference. E.g. we can plot the posterior density, get point estimates -->
<!-- and interval estimates by extracting the posterior mean and credible -->
<!-- intervals. -->

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jags.mod.infantDeath <- function(){
for(i in 1:12){
r[i] ~ dbin(theta[i],n[i])
logit(theta[i]) <- logit.theta[i]
logit.theta[i] ~ dnorm(mu, tau)
}
mu ~ dnorm(0,0.001)
tau <- 1/sigma^2
sigma ~ dunif(0,100)
}

# Parameters to monitor
jags.param.infantDeath <- c("mu", "sigma", "theta", "tau")

## initial values
inits1 <- list('tau' = 100, 'sigma' = 20, 'theta'=c(10,-5,-25))
inits2 <- list('tau'=100000,'sigma'=-100,'theta'=c(-100,100,500))

jags.inits = list(inits1,inits2)

## data

jags.data.infantDeath <- list(r = surgicalDF$r, n = surgicalDF$n)

## fit the jags model

jags.mod.fit.infantDeath <- jags(data = jags.data.infantDeath, #inits = jags.inits,
parameters.to.save = jags.param.infantDeath, n.chains = 2, n.iter = 10000,
n.burnin = 5000,n.thin=1,model.file = jags.mod.infantDeath, DIC=FALSE)


```
Now that we have the fitted the model we will observe the plots to check for possible convergence during the recorded 5000 iterations.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jagsfitinfantDeath.mcmc <- as.mcmc(jags.mod.fit.infantDeath )
MCMCtrace(jagsfitinfantDeath.mcmc ,type = 'trace',ind = TRUE, pdf = FALSE)


```

As we can see from the plots, there seems to be no visible patterns on the chains and all parameters have stable oscillation around a common value. We can also observe that the chains seem to be well mixed with each other as it is difficult to distinguish between chains.

However visualization alone is not enough to fully access the convergence so we will use the Gelman-Rubin diagnostics to check for convergence.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

gelman.diag(jagsfitinfantDeath.mcmc)

```
As we can see from the Upper Confidence Interval, the values of all parameters are approximately 1, indicating that the parameters have indeed converged.


### Question 2 b)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jags.mod.infantDeath <- function(){
for(i in 1:12){
r[i] ~ dbin(theta[i],n[i])
logit(theta[i]) <- logit.theta[i]
logit.theta[i] ~ dnorm(mu, tau)
}
mu ~ dnorm(0,0.001)
tau <- 1/sigma^2
sigma ~ dunif(0,100)

## pi = P (rpred i > ri) + 1/2 * P (rpred i = ri)



}

# Parameters to monitor
jags.param.infantDeath <- c("mu", "sigma", "theta", "tau")

## initial values
inits1 <- list('tau' = 100, 'sigma' = 20, 'theta'=c(10,-5,-25))
inits2 <- list('tau'=100000,'sigma'=-100,'theta'=c(-100,100,500))

jags.inits = list(inits1,inits2)

## data

jags.data.infantDeath <- list(r = surgicalDF$r, n = surgicalDF$n)

## fit the jags model

jags.mod.fit.infantDeath <- jags(data = jags.data.infantDeath, #inits = jags.inits,
parameters.to.save = jags.param.infantDeath, n.chains = 2, n.iter = 10000,
n.burnin = 5000,n.thin=1,model.file = jags.mod.infantDeath, DIC=FALSE)


```

### Question 2 c)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}




```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}




```

## Question B - classification

## Question B.1

Group 0 is characterised for always having x1 \> -1,65 and x2 \< 2,05
with the majority of the occurrences of group 0 having

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## I am just going to split the dataframe in two so its easier to show an histogram of each variable for both groups

group0 = subset(classificationDF, Group == 0)
group1 = subset(classificationDF, Group == 1)

##removing the id variable from both of these new data frames so they are not included in the histograms
group0 = group0 %>% select(-(1))
group1 = group1 %>% select(-(1))


plot_histogram(group0)


```

As we can see from the histograms from group 0 it seems that X2 seems to
follow a normal distribution that is very slightly skewed to the left
and has mean -0,5. X1 is clearly skewed to the right possibly following
a normal distribution and again mean equal to -0,5.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

plot_histogram(group1)


```

Group 1 histograms reveal that X2 is normally distributed with mean
equal to 0,5 and sightly skewed to the left. The variable X1 also seems
to follow a normal distribution that has a very prolonged skewness to
the left and mean 0,5.0

## Question B.2

Both LDA and QDA assume normality which is true from what we can see
from the histograms on 2.1

Starting with Linear discriminant analysis (LDA) we can easily see from
the initial graph that we can see that the any sensible decision
boundary is highly non-linear. As analysed from histograms this data
violates the initial assumption of homogeneity of variance
(homoscedasticity). The non-linearity of the data would also lead to LDA
having terrible performance. As such we can conclude that LDA is not
suitable for this data.

Secondly, quadratic discriminant analysis (QDA) follow the assumption of
normality of each of the variables which does seem to hold even with how
highly non-linear the data is. However judging from the plot with the
observations, it does seem that QDA might not have enough flexibility to
account for all the variability in what is the theoretical Bayes
decision boundary. As such QDA could be appropriate for this data but
not the best performing classification method for such data.

Thirdly, K-nearest neighbour classification (KNN) has no assumptions
made about the shape of the decision, as such as we choose a adequate
level of smoothness to prevent overfitting KNN classification is an
adequate method for the data.

Support Vector Machines (SVM) are most commonly used for binary
classification, which holds true for our data. The high non-linear
decision boundary however requires a careful selection of a non linear
kernel function to ensure that the boundary becomes non-linear when
converted back to the regular space. As such I deem SVM an appropriate
method to classify the data.

Finally, Random forest works by constructing multiple decision trees
randomly selecting a portion of the variables and de-correlate them
using multiple bootstrapped sets. Since our data has only 2 variables
there isn't enough variables to overcome overfitting. To conclude,
Random Forest classification is not very appropriate to this type of
data.

## Question B.3

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# make this example reproducible
set.seed(26041999)
# use 80% of dataset as training set and 20% as test set
sample = sample.split(classificationDF$Group, SplitRatio = 0.8)
## note that the column selected above can be any column.
train = classificationDF[sample, ]
test = classificationDF[!sample, ]

# split the data using the indices returned by the createDataPartition function
X_train = train[, c("X1", "X2")]
y_train = train$Group
X_test = test[, c("X1", "X2")]
y_test = test$Group

# check the dimensions 
dim(train)
dim(test)


```

## Question B.4

The 3 method I will be using are Random forest, KNN and SVM as deemed to be the
most appropriate methods for this data.

### Random Forest


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, output=FALSE}

trainNum = train
trainNum$Group = as.factor(trainNum$Group)

trainNum = trainNum %>% select(-(1))

```

For random Forest we will tune the number of variables randomly split for each of the random samples, as such we will try if only 1 split variable or both variables give a better accuracy

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Random Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
rf_random <- train(Group~., data=trainNum, method="rf", metric="Accuracy", tuneLength=15, trControl=control)
print(rf_random)
plot(rf_random)

```
As we can see the accuracy is almost identical with a difference of less that 0,1% as such I will use mtry = 2 as it is more accurate.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Train the model
rf_model <- randomForest(Group ~ ., data=train, 
                         tuneGrid=data.frame(mtry=2))


# Make predictions on the test data
predictions <- predict(rf_model, test)

## Manually making the cut at 0,5 of predicition for the groups
predictions  <- ifelse(predictions >= 0.5, 1, 0)

confusionMatrix(as.factor(predictions), as.factor(y_test))


```

### KNN - K-nearest neighbour

First we make an graph displaying the accuracy for each k from 1 to 30 to see which K is the most appropriate by giving us the best accuracy.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Define range of K values to test
k_range <- 1:30

# Empty list to store validation accuracy scores
accuracy_scores <- c()

# Train and evaluate KNN model for each value of K
for (k in k_range) {
  knn <- knn(X_train, X_test, y_train, k = k)
  accuracy_scores <- c(accuracy_scores, sum(knn == y_test)/length(y_test))
}

# Plot the elbow curve
df <- data.frame(k = k_range, accuracy = accuracy_scores)
ggplot(df, aes(x = k, y = accuracy)) +
  geom_line() +
  geom_point() +
  xlab("K") +
  ylab("Validation Accuracy") +
  ggtitle("Elbow Curve for KNN")

```

As we can see from the graph, k= 9 is the one that give us the most accuracy and most other values have not been as highly accurate.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# fit the model
fit = knn(train=X_train, test=X_test, cl=y_train,k=9)
# fit = knn(xTrain,xTest,yTrain,k=3)

# produce the confusion matrix
# when numbers are used as class labels we might need
confusion = confusionMatrix(as.factor(fit),as.factor(y_test))

confusion

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE}

test$predicted_group = fit
test$correct_prediction = test$predicted_group == test$Group

# plot the data points
p = ggplot() + 
  geom_point(data = train, aes(x = X1, y = X2, color = "Training"), size = 3) +
  geom_point(data = test, aes(x = X1, y = X2, color = correct_prediction, shape = factor(correct_prediction)), size = 3) +
  scale_color_manual(values = c("red", "blue", "green"), labels = c("Incorrect", "Correct", "Training")) +
  scale_shape_manual(values = c(1, 16)) +
  labs(x = "X1", y = "X2", color = "", shape = "") +
  ggtitle("KNN Classification Results")

# display the plot
print(p)

```

### SVM - Support Vector Machines

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), warning=FALSE}

## checking the model performance with multiple difference C constants
mdl = train(x=X_train,y=y_train, method = "svmLinear",
trControl = trainControl("cv", number = 5),
tuneGrid = expand.grid(C = seq(0, 2, length = 20)))
# Plot model accuracy vs different values of Cost
plot(mdl)
# Print the best tuning parameter C that maximises model accuracy
mdl$bestTune



```

#TODO talk about how there is a balance on how the number of miss classified

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE, echo=FALSE}

## the SVM not working part

# # Test model on testing data
# yTestPred = predict(mdl, newdata=X_test)
# confusionMatrix(yTestPred, y_test) # predicted/true

```


## TODO this is using radial but I should have one with polynimial as well to show that polinomial is better

```{r, warning=FALSE}

## https://github.com/MatheusSchaly/Online-Courses/blob/master/Machine_Learning_A-Z_Hands-On_Python_%26_R_In_Data_Science/2_Classification/R/4_Kernel_SVM.R
## to add visualization

mdl = train(x=X_train,y=y_train, method='svmRadial') 
print(mdl)

mdl = svm(Group ~ .,
                 data = train,
                 type = 'C-classification',
                 kernel = 'radial')

# Test model on testing data
yTestPred = predict(mdl, newdata=test)
# yTestPred = mdl %>% predict(xTest) 
confusionMatrix(as.factor(yTestPred), as.factor(y_test)) # predicted/true


```


## Question B.5


The best method is ... because ...