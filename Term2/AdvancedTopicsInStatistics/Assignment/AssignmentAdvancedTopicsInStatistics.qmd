---
title: "AdvancedStatsAssignment"
format: pdf
editor: visual
---

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, output=FALSE}

library(C50)
library(caret) # function for creating train/test datasets, and confusion matrices
library(class) # this has the knn function
library(ggplot2)
library(tidyverse)
library(tidyr)
library(DataExplorer)
library(caTools)
library(kernlab)
library(e1071)


library(RColorBrewer)
library(sf)
library(rgdal)

#install.packages("sf")

library(reshape2)

library(R2jags); library(MCMCvis); library(coda); library(lattice)

classificationDF = read_csv("Classification.csv")

ScotlandDF = read_csv("Scotland.csv")

surgicalDF = read_csv("surgical.csv")

## https://www.r-bloggers.com/2012/04/getting-started-with-jags-rjags-and-bayesian-modelling/

```

## Question 1

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE}

source("ScotlandMap.R") # need to read in the Scotland Map function
testdat = runif(56) # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland random numbers")

```

## Question 1 a)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

ScotlandDF$SMR = ScotlandDF$Observed /ScotlandDF$Expected

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), warning=FALSE}

hist(ScotlandDF$SMR, breaks = 20, col = "lightblue",
     main = "Distribution of Standard Mortality Ratios",
     xlab = "SMR")

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat = ScotlandDF$SMR # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland random numbers")


```

## Question 1 b)

For the model: \begin{align*}
Obs_i \sim \text{Pois}(\mu_i) \
\log(\mu_i) = \log(\text{Exp}_i) + \beta_0 + \theta_i \
\text{RR}_i = \exp(\beta_0) \exp(\theta_i)
\end{align*}= \log(\text{Exp}\_i) + \beta\_0 + \theta\_i\
\text{RR}\_i = \exp(\beta\_0) \exp(\theta\_i) \\end{align\*}=
\log(\text{Exp}\_i) + \beta\_0 + \theta\_i\
\text{RR}\_i = \exp(\beta\_0) \exp(\theta\_i) \\end{align\*}

$\beta0$ is the intercept that represents the baseline for when all the
other variables are 0 which can also be interpreted as the log number
expected of cases per administrative area.

$\theta i$ is the effect that captures the variation in the observed
number of cases that cannot be explained by the expected numbers alone.
$\theta i$ also represents the deviation of the log observed number of
cases from the log expected number of cases for area i.

In the Poisson model, $\theta i$ represents the deviation of the
observed number of cases in area i from the expected number of cases in
area i, given the reference rates. In other words, it captures how much
higher or lower the observed number of cases is compared to what we
would expect based on the reference rates

The role of Î¸i in the estimation of the relative risk is to allow for
variation in the risk of disease across different areas, over and above
what is explained by the reference rates. By including a random effect
for each area in the model, we are able to estimate the relative risk
for each area, while accounting for the fact that some areas may have
higher or lower risk due to factors that are not captured by the
reference rates alone.

## Question 1 c)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# # Define the JAGS model
# model = "
# model {
#   # Priors
#   beta0 ~ dnorm(0, 1/1000)
#   tau ~ dgamma(1, 0.05)
# 
#   # Likelihood
#   for (i in 1:N) {
#     obs[i] ~ dpois(mu[i])
#     log(mu[i]) = log(expo[i]) + beta0 + theta[i]
#     theta[i] ~ dnorm(0, tau)
#   }
# }

# model definition
jags.mod.poisson = function(){
  # Priors
  beta0 ~ dnorm(0, 1/1000)
  tau ~ dgamma(1, 0.05)
  
  # Likelihood
  for (i in 1:N) {
    obs[i] ~ dpois(mu[i])
    log(mu[i]) = log(expo[i]) + beta0 + theta[i]
    theta[i] ~ dnorm(0, tau)

  }
}  

# Data
list(N = nrow(ScotlandDF),
     obs = ScotlandDF$Observed,
     expo = ScotlandDF$Expected)

# Parameters to monitor
params = c("beta0", "theta", "tau")

# Number of chains
n.chains = 2.

# Number of iterations
n.iter = 5000

# Burn-in
burnin = 1000

# Run the model
results = run.jags(jags.mod.poisson, n.chains = n.chains, n.iter = n.iter, burnin = burnin,
                    monitor = params, method = "parallel")
                    
# Check convergence
plot(results)
summary(results)


```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Number of areas
n_areas <- nrow(ScotlandDF)



# Define model string
model_string <- "
  model {
    for (i in 1:n_areas) {
      obs[i] ~ dpois(mu[i])
      mu[i] <- exp(beta0 + theta[i]) * offset[i]
      log(offset[i]) <- log(expec[i])
      theta[i] ~ dnorm(0, tau)
    }
    beta0 ~ dnorm(0, 1/1000)
    tau ~ dgamma(1, 0.05)
  }
"

# Define model parameters
parameters <- c("beta0", "tau", "theta", "RR")

# Define initial values
initial_values <- list(
  beta0 = 0,
  tau = 20,
  theta = rnorm(n_areas, 0, sqrt(1/tau)),
  RR = rep(1, n_areas)
)

# Compile the model
model <- jags.model(textConnection(model_string), data = list(obs = ScotlandDF$obs, expec = ScotlandDF$expec, n_areas = n_areas), n.chains = 2)

# Run the model
n.iter <- 10000
burnin <- 1000
thin <- 10
model.samples <- coda.samples(model, variable.names = parameters, n.iter = n.iter, thin = thin)

# Plot trace plots
traceplot(model.samples, varname.cex = 0.7)

# Summarise model parameters
summary(model.samples)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Define model in JAGS
modelString <- "model{
  # Priors
  beta0 ~ dnorm(0, 1/1000)
  tau ~ dgamma(1, 0.05)

  # Likelihood
  for(i in 1:n_areas){
    theta[i] ~ dnorm(0, tau)
    obs[i] ~ dpois(exp(beta0 + logExp[i] + theta[i]))
  }

  # Offset term
  for(i in 1:n_areas){
    logExp[i] <- log(offset[i])
  }
}"

# Compile model
model <- jags.model(textConnection(modelString), data = list(obs = ScotlandDF$Observed,
                                                           offset = ScotlandDF$Expected,
                                                           n_areas = nrow(ScotlandDF)),
                    n.chains = 2)

# Burn-in and run MCMC
n.iter <- 20000
n.burnin <- 100000
params <- c("beta0", "theta")
jags.out <- coda.samples(model, variable.names = params, n.iter = n.iter+n.burnin, thin = 1)

# Summarize results
summary(jags.out)

# Trace plots
traceplot(jags.out)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

## Question 2 a)

<!-- To main steps of fitting a Bayesian model using R2jags are the -->
<!-- following. 1. Define the JAGS model (as a function). 2. Prepare the -->
<!-- data, which should be passed on as a list. Note that this list should -->
<!-- include other known variables not in the data, like the length N of the -->
<!-- data vector, when this N is used in the JAGS model definition (e.g. in a -->
<!-- for loop). 3. Identify the parameters whose posterior distributions are -->
<!-- of interest. Here a vector of the parameter names is needed. 4. Define -->
<!-- starting values for JAGS. We need starting values for all the stochastic -->
<!-- nodes (unknown parameters, missing data) in each chain. Initial values -->
<!-- should be passed on as a list. Note that JAGS runs without user-defined -->
<!-- starting values, in which case it generates its own initial values. But -->
<!-- this is not recommended, especially for complex models. 5. Fit the model -->
<!-- in JAGS. The most important arguments of the jags function includes: -->
<!-- data (data list), inits (list of initial values), parameters.to.save -->
<!-- (parameters of interest), n.chains (number of MC chains - each gives a -->
<!-- sequence of simulated values), n.iter (number of iterations in the -->
<!-- sampling process), n.burnin (number of initial samples to discard), -->
<!-- model.file (the JAGS model definition). 6. Convergence diagnostics. -->
<!-- Update if convergence hasn't reached. (See later). 7. Once we are -->
<!-- satisfied that the chains have converged, we can start the statistical -->
<!-- inference. E.g. we can plot the posterior density, get point estimates -->
<!-- and interval estimates by extracting the posterior mean and credible -->
<!-- intervals. -->

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jags.mod.infantDeath <- function(){
for(i in 1:12){
r[i] ~ dbin(theta[i],n[i])
logit(theta[i]) <- logit.theta[i]
logit.theta[i] ~ dnorm(mu, tau)
}
mu ~ dnorm(0,0.001)
tau <- 1/sigma^2
sigma ~ dunif(0,100)
}

# Parameters to monitor
jags.param.infantDeath <- c("mu", "sigma", "theta")

## initial values
inits1 <- list('tau' = 100, 'sigma' = 20, 'theta'=c(10,-5,-25))
inits2 <- list('tau'=100000,'sigma'=-100,'theta'=c(-100,100,500))

jags.inits = list(inits1,inits2)

## fit the jags model

jags.data.infantDeath <- list(r = surgicalDF$r, n = surgicalDF$n)

jags.mod.fit.infantDeath <- jags(data = jags.data.infantDeath, #inits = jags.inits,
parameters.to.save = jags.param.infantDeath, n.chains = 2, n.iter = 10000,
n.burnin = 5000,n.thin=1,model.file = jags.mod.infantDeath, DIC=FALSE)


```
Now that we have the fitted the model we will observe the plots to check for possible convergence during the recorded 5000 iterations.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jagsfitinfantDeath.mcmc <- as.mcmc(jags.mod.fit.infantDeath )
MCMCtrace(jagsfitinfantDeath.mcmc ,type = 'trace',ind = TRUE, pdf = FALSE)


```

As we can see from the plots, there seems to be no visible patterns on the chains and all parameters have stable oscillation around a common value. We can also observe that the chains seem to be well mixed with each other as it is difficult to distinguish between chains.

However visualization alone is not enough to fully access the convergence so we will use the Gelman-Rubin diagnostics to check for convergence.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

gelman.diag(jagsfitinfantDeath.mcmc)

```
As we can see from the Upper Confidence Interval, the values of all parameters are approximately 1, indicating that the parameters have indeed converged.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}




```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}




```

## Question B - classification

## Question B.1

Group 0 is characterised for always having x1 \> -1,65 and x2 \< 2,05
with the majority of the occurences of group 0 having

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## I am just going to split the dataframe in two so its easier to show an histogram of each variable for both groups

group0 = subset(classificationDF, Group == 0)
group1 = subset(classificationDF, Group == 1)

##removing the id variable from both of these new data frames so they are not included in the histograms
group0 = group0 %>% select(-(1))
group1 = group1 %>% select(-(1))


plot_histogram(group0)


```

As we can see from the histograms from group 0 it seems that X2 seems to
follow a normal distribution that is very slightly skewed to the left
and has mean -0,5. X1 is clearly skewed to the right possibly following
a normal distribution and again mean equal to -0,5.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

plot_histogram(group1)


```

Group 1 histograms reveal that X2 is normally distributed with mean
equal to 0,5 and sightly skewed to the left. The variable X1 also seems
to follow a normal distribution that has a very prolonged skewness to
the left and mean 0,5.0

## Question B.2

Both LDA and QDA assume normality which is true from what we can see
from the histograms on 2.1

Starting with Linear discriminant analysis (LDA) we can easily see from
the initial graph that we can see that the any sensible decision
boundary is highly non-linear. As analysed from histograms this data
violates the initial assumption of homogeneity of variance
(homoscedasticity). The non-linearity of the data would also lead to LDA
having terrible performance. As such we can conclude that LDA is not
suitable for this data.

Secondly, quadratic discriminant analysis (QDA) follow the assumption of
normality of each of the variables which does seem to hold even with how
highly non-linear the data is. However judging from the plot with the
observations, it does seem that QDA might not have enough flexibility to
account for all the variability in what is the theoretical Bayes
decision boundary. As such QDA could be appropriate for this data but
not the best performing classification method for such data.

Thirdly, K-nearest neighbour classification (KNN) has no assumptions
made about the shape of the decision, as such as we choose a adequate
level of smoothness to prevent overfitting KNN classification is an
adequate method for the data.

Support Vector Machines (SVM) are most commonly used for binary
classification, which holds true for our data. The high non-linear
decision boundary however requires a careful selection of a non linear
kernel function to ensure that the boundary becomes non-linear when
converted back to the regular space. As such I deem SVM an appropriate
method to classify the data.

Finally, Random forest works by constructing multiple decision trees
randomly selecting a portion of the variables and de-correlate them
using multiple bootstrapped sets. Since our data has only 2 variables
there isn't enough variables to overcome overfitting. To conclude,
Random Forest classification is not very appropriate to this type of
data.

## Question B.3

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# make this example reproducible
set.seed(26041999)
# use 80% of dataset as training set and 20% as test set
sample = sample.split(classificationDF$Group, SplitRatio = 0.8)
## note that the column selected above can be any column.
train = classificationDF[sample, ]
test = classificationDF[!sample, ]

# split the data using the indices returned by the createDataPartition function
X_train = train[, c("X1", "X2")]
y_train = train$Group
X_test = test[, c("X1", "X2")]
y_test = test$Group

# check the dimensions 
dim(train)
dim(test)


```

## Question B.4

The 3 method I will be using are QDA, KNN and SVM as deemed to be the
most appropriate methods for this data.

## QDA - quadratic discriminant analysis

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

## KNN - K-nearest neighbour

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# fit the model
fit = knn(train=X_train, test=X_test, cl=y_train,k=3)
# fit = knn(xTrain,xTest,yTrain,k=3)

# produce the confusion matrix
# when numbers are used as class labels we might need
confusion = confusionMatrix(as.factor(fit),as.factor(y_test))

confusion

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

test$predicted_group = fit
test$correct_prediction = test$predicted_group == test$Group

# plot the data points
p = ggplot() + 
  geom_point(data = train, aes(x = X1, y = X2, color = "Training"), size = 3) +
  geom_point(data = test, aes(x = X1, y = X2, color = correct_prediction, shape = factor(correct_prediction)), size = 3) +
  scale_color_manual(values = c("red", "blue", "green"), labels = c("Incorrect", "Correct", "Training")) +
  scale_shape_manual(values = c(1, 16)) +
  labs(x = "X1", y = "X2", color = "", shape = "") +
  ggtitle("KNN Classification Results")

# display the plot
print(p)

```

## SVM - Support Vector Machines

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## checking the model performance with multiple difference C constants
mdl = train(x=X_train,y=y_train, method = "svmLinear",
trControl = trainControl("cv", number = 5),
tuneGrid = expand.grid(C = seq(0, 2, length = 20)))
# Plot model accuracy vs different values of Cost
plot(mdl)
# Print the best tuning parameter C that maximises model accuracy
mdl$bestTune



```

#TODO talk about how there is a balance on how the number of miss
classified

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## the SVM not working part

# # Test model on testing data
# yTestPred = predict(mdl, newdata=X_test)
# confusionMatrix(yTestPred, y_test) # predicted/true

```

## quick debug que funciona CARALHOOOOOOOOOOOOOOOO

## TODO this is using radial but I should have one with polynimial as well to show that polinomial is better

```{r, warning=FALSE}

## https://github.com/MatheusSchaly/Online-Courses/blob/master/Machine_Learning_A-Z_Hands-On_Python_%26_R_In_Data_Science/2_Classification/R/4_Kernel_SVM.R
## to add visualization

mdl = train(x=X_train,y=y_train, method='svmRadial') 
print(mdl)

mdl = svm(Group ~ .,
                 data = train,
                 type = 'C-classification',
                 kernel = 'radial')

# Test model on testing data
yTestPred = predict(mdl, newdata=test)
# yTestPred = mdl %>% predict(xTest) 
confusionMatrix(as.factor(yTestPred), as.factor(y_test)) # predicted/true


```


## Question B.5