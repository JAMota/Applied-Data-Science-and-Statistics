---
title: "AdvancedStatsAssignment"
format:
  pdf:
    toc: true
    toc-depth: 4
editor: visual
---

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, output=FALSE}

rm(list = ls())

library(C50)
library(caret) # function for creating train/test datasets, and confusion matrices
library(class) # this has the knn function
library(ggplot2)
library(tidyverse)
library(tidyr)
library(DataExplorer)
library(caTools)
library(kernlab)
library(e1071)

library(randomForest)
library(mlbench)

library(stringr)

library(RColorBrewer)
library(sf)
library(rgdal)

#install.packages("sf")

library(reshape2)

library(R2jags); library(MCMCvis); library(coda); library(lattice)

classificationDF = read_csv("Classification.csv")

ScotlandDF = read_csv("Scotland.csv")

surgicalDF = read_csv("surgical.csv")

## https://www.r-bloggers.com/2012/04/getting-started-with-jags-rjags-and-bayesian-modelling/

```

## Question 1

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE}

source("ScotlandMap.R") # need to read in the Scotland Map function
testdat = runif(56) # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland random numbers")

```

### Question 1 a)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

ScotlandDF$SMR = ScotlandDF$Observed /ScotlandDF$Expected

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), warning=FALSE}

plot(density(ScotlandDF$SMR), main = "Distribution of Standard Mortality Ratios", 
     xlab = "SMR", ylab = "Density", col = "red")

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat = ScotlandDF$SMR # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland SMR by region")


```

### Question 1 b)

<!-- ##fix this latex TODO -->
<!-- For the model: \begin{align*} -->
<!-- Obs_i \sim \text{Pois}(\mu_i) \ -->
<!-- \log(\mu_i) = \log(\text{Exp}_i) + \beta_0 + \theta_i \ -->
<!-- \text{RR}_i = \exp(\beta_0) \exp(\theta_i) -->
<!-- \end{align*}= \log(\text{Exp}\_i) + \beta\_0 + \theta\_i\ -->
<!-- \text{RR}\_i = \exp(\beta\_0) \exp(\theta\_i) \\end{align\*}= -->
<!-- \log(\text{Exp}\_i) + \beta\_0 + \theta\_i\ -->
<!-- \text{RR}\_i = \exp(\beta\_0) \exp(\theta\_i) \\end{align\*} -->

$\beta0$ is the intercept that represents the baseline for when all the
other variables are 0 which can also be interpreted as the log number
expected of cases per administrative area.

$\theta i$ is the effect that captures the variation in the observed
number of cases that cannot be explained by the expected numbers alone.
$\theta i$ also represents the deviation of the log observed number of
cases from the log expected number of cases for area i.

In the Poisson model, $\theta i$ represents the deviation of the
observed number of cases in area i from the expected number of cases in
area i, given the reference rates. In other words, it captures how much
higher or lower the observed number of cases is compared to what we
would expect based on the reference rates

The role of θi in the estimation of the relative risk is to allow for
variation in the risk of disease across different areas, over and above
what is explained by the reference rates. By including a random effect
for each area in the model, we are able to estimate the relative risk
for each area, while accounting for the fact that some areas may have
higher or lower risk due to factors that are not captured by the
reference rates alone.

### Question 1 c)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jags.mod.lipCancer = function(){
  # Priors
  beta0 ~ dnorm(0, 1/1000)
  tau ~ dgamma(1, 0.05)
  
  # Likelihood
  for (i in 1:N) {
    obs[i] ~ dpois(mu[i])
    log(mu[i]) = log(expo[i]) + beta0 + theta[i]
    theta[i] ~ dnorm(0, tau)
    RR[i] = exp(beta0) * exp(theta[i])
  }
} 


# Parameters to monitor
jags.param.lipCancer = c("beta0", "theta", "tau", "RR")

# Data
jags.data.lipCancer <- list(N = nrow(ScotlandDF),
     obs = ScotlandDF$Observed,
     expo = ScotlandDF$Expected)

## initial values, have to be for the stochaic for thetta it will be a repetition because of the i
inits1 <- list('tau' = 1, 'beta0' = 1, 'theta'= rep(1,56))
inits2 <- list('tau'=17.20806,'beta0'=-0.5604756,'theta'=rep(19.38524,56))

## these initial values for inits2 came from these functions random generation set.seed(123)
#y <- rgamma(n = 1, shape = 1, rate = 0.05)
#value <- rnorm(1, mean = 0, sd = sqrt(1000))
#tau_init <- 1/rgamma(n = 1, shape = 2, rate = 0.1)


jags.inits.lipCancer = list(inits1,inits2)

## fit the jags model

jags.mod.fit.lipCancer <- jags(data = jags.data.lipCancer, inits = jags.inits.lipCancer,
parameters.to.save = jags.param.lipCancer, n.chains = 2, n.iter = 15000,
n.burnin = 7500,n.thin=1,model.file = jags.mod.lipCancer, DIC=FALSE)



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jagsfitlipCancer.mcmc <- as.mcmc(jags.mod.fit.lipCancer )
MCMCtrace(jagsfitlipCancer.mcmc ,type = 'trace',ind = TRUE, pdf = FALSE)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

gelman.diag(jagsfitlipCancer.mcmc)


```

### question 1.d)

we know have to monitor RR as well 

<!-- # extract posterior mean for mu -->
<!-- pos <- substr(rownames(jags.mod.fit.lipCancer$BUGSoutput$summary),1,2)=='mu' -->

<!-- muRegions = jags.mod.fit.lipCancer$BUGSoutput$summary[pos,1] -->

<!-- ScotlandDF$mu = muRegions -->

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

##extract posterior RR
posRR <- substr(rownames(jags.mod.fit.lipCancer$BUGSoutput$summary),1,2)=='RR'

RRRegions = jags.mod.fit.lipCancer$BUGSoutput$summary[posRR,1]

ScotlandDF$RR = RRRegions

```

Checking the difference between the average RR and the SMR for each region 

I firstly do RR/SMR and see how close it is to 1

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

ScotlandDF$SMR / ScotlandDF$RR

```
And now I plot it on the map to see how close the posterior is to SMR likelihood

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

comparison = ScotlandDF$SMR / ScotlandDF$RR

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat = comparison # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland SMR by RR comparison by region")


```


### Question 1.e)

Now I will have to check when RR is > 1 using similarly the summary Confidicence intervals

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jags.mod.lipCancer = function(){
  # Priors
  beta0 ~ dnorm(0, 1/1000)
  tau ~ dgamma(1, 0.05)
  
  # Likelihood
  for (i in 1:N) {
    obs[i] ~ dpois(mu[i])
    log(mu[i]) = log(expo[i]) + beta0 + theta[i]
    theta[i] ~ dnorm(0, tau)
    RR[i] = exp(beta0) * exp(theta[i])
  }
  
  PRRAbove = ifelse(RR>1,1,0)
  
} 

# Parameters to monitor
jags.param.lipCancer = c("beta0", "theta", "tau", "RR", "PRRAbove")


# Data
jags.data.lipCancer <- list(N = nrow(ScotlandDF),
     obs = ScotlandDF$Observed,
     expo = ScotlandDF$Expected)

## initial values, have to be for the stochaic for thetta it will be a repetition because of the i
inits1 <- list('tau' = 1, 'beta0' = 1, 'theta'= rep(1,56))
inits2 <- list('tau'=17.20806,'beta0'=-0.5604756,'theta'=rep(19.38524,56))

## these initial values for inits2 came from these functions random generation set.seed(123)
#y <- rgamma(n = 1, shape = 1, rate = 0.05)
#value <- rnorm(1, mean = 0, sd = sqrt(1000))
#tau_init <- 1/rgamma(n = 1, shape = 2, rate = 0.1)


jags.inits.lipCancer = list(inits1,inits2)

## fit the jags model

jags.mod.fit.lipCancer <- jags(data = jags.data.lipCancer, inits = jags.inits.lipCancer,
parameters.to.save = jags.param.lipCancer, n.chains = 2, n.iter = 15000,
n.burnin = 7500,n.thin=1,model.file = jags.mod.lipCancer, DIC=FALSE)




```



```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## to see which regions RR are in the 95% C.I we will have to see if the 2.5% is small or equal to 1 and if 97.5% is equal or greater than 1

##extract posterior RR
posProbRR <- substr(rownames(jags.mod.fit.lipCancer$BUGSoutput$summary),1,2)=='RR'

#jags.mod.fit.lipCancer$BUGSoutput$summary[posProbRR,1] ##mean
lowerCI = jags.mod.fit.lipCancer$BUGSoutput$summary[posProbRR,3] # 2.5 percentile
upperCI = jags.mod.fit.lipCancer$BUGSoutput$summary[posProbRR,7] # 97.5 percentile


```

Since we know that the 97,5 percentile is always bigger than the the 2,5 percentile to check if the 95% CI of RR is lower than 1, meaning there is no excessive risk,  all we have to do is check if a region 97,5% percentile is smaller than 1


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

ScotlandDF$AboveRR = upperCI
ScotlandDF$isAboveRR = 0

for(i in 1:nrow(ScotlandDF) ){

  if(ScotlandDF$AboveRR[i] > 1){
    ScotlandDF$isAboveRR[i] = 1
  }
  
  
  if(upperCI[i] < 1){
    print(ScotlandDF$Name[i])
  }
}


```
Here is the list of the regions where we are 95% certain that RR is bellow 1

So now to map the regions with and without excess risk of lip cancer

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat = ScotlandDF$isAboveRR # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland regions with excess lip cancer")

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat = as.integer(!ScotlandDF$isAboveRR) # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland regions without excess of lip cancer")

```

Here I have decided to map separetly without and without the risk of lip cancer due to the default colour scheme of the above risk cancer regions making it very hard to see which regions have risk bellow 1.



### Question 1f)

For the new priors I have chosen to utilize the previous posterior values for $\beta0$ and $\tau$ as they represent our newly updated beliefs about the values derived from the data after the first model.

Here, we have used the posterior distributions from the previous model of the same data to update our prior beliefs about the parameters $\beta0$ and $\tau$ and as such this new model has information about our previous analysis. 
The mean of the posterior distributions is an approximated value of the most likely values for $\beta0$ and $\tau$ from the observed data.

We will first start by gathering the previous posteriors to use as the new model priors.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}


  # Extracting the posterior mean for each variable
  beta0PosteriorMean <- mean(as.numeric(jags.mod.fit.lipCancer$BUGSoutput$sims.list$beta0))
  tauPosteriorMean <- mean(as.numeric(jags.mod.fit.lipCancer$BUGSoutput$sims.list$tau))

  beta0PosteriorSD = sd(as.numeric(jags.mod.fit.lipCancer$BUGSoutput$sims.list$beta0))
  tauPosteriorSD = sd(as.numeric(jags.mod.fit.lipCancer$BUGSoutput$sims.list$tau))
  
  
  beta0PosteriorMean
  beta0PosteriorSD
  
  tauPosteriorMean
  tauPosteriorSD

```
  <!-- #beta0.samples = as.numeric(jags.mod.fit.lipCancer$BUGSoutput$sims.list$beta0) -->
  <!-- #tau.samples = as.numeric(jags.mod.fit.lipCancer$BUGSoutput$sims.list$tau) -->
  <!-- beta0PosteriorMean <- mean(beta0.samples) -->
  <!-- beta0PosteriorSD <- sd(beta0.samples) -->
  <!-- tauPosteriorAlpha <- (mean(tau.samples)^2) / var(tau.samples) -->
  <!-- tauPosteriorBeta <- mean(tau.samples) / var(tau.samples) -->
  <!--   tau ~ dgamma(tauPosteriorAlpha, tauPosteriorBeta) -->
  <!-- beta0 ~ dnorm(beta0PosteriorMean, beta0PosteriorSD) -->
  
Here I will supply the values of the mean and sd from both beta0 and tau manually since JAGS model function does not have a global scope to see all variables in the system

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jags.mod.lipCancerNew = function(beta0PosteriorMean){
  
  ##calcs for prior gamma
  ## α = (x_bar^2 / s^2)
  ## β = (s^2 / x_bar)
  shape <- (1.811285/0.45581)^2
  rate <- ((0.45581)^2) /1.811285
  
  # Priors
  beta0 ~ dnorm(0.08064184, 0.1167079)

  tau ~ dgamma(shape, rate)

  
  # Likelihood
  for (i in 1:N) {
    obs[i] ~ dpois(mu[i])
    log(mu[i]) = log(expo[i]) + beta0 + theta[i]
    theta[i] ~ dnorm(0, tau)
    RR[i] = exp(beta0) * exp(theta[i])
  }
} 


# Parameters to monitor
jags.param.lipCancerNew = c("beta0", "theta", "tau", "RR")

# Data
jags.data.lipCancerNew <- list(N = nrow(ScotlandDF),
     obs = ScotlandDF$Observed,
     expo = ScotlandDF$Expected)

## initial values, have to be for the stochaic for thetta it will be a repetition because of the i
inits1 <- list('tau' = 1, 'beta0' = 1, 'theta'= rep(1,56))
inits2 <- list('tau'=17.20806,'beta0'=-0.5604756,'theta'=rep(19.38524,56))

## these initial values for inits2 came from these functions random generation set.seed(123)
#y <- rgamma(n = 1, shape = 1, rate = 0.05)
#value <- rnorm(1, mean = 0, sd = sqrt(1000))
#tau_init <- 1/rgamma(n = 1, shape = 2, rate = 0.1)


jags.inits.lipCancerNew = list(inits1,inits2)

## fit the jags model

jags.mod.fit.lipCancerNew <- jags(data = jags.data.lipCancerNew, inits = jags.inits.lipCancerNew,
parameters.to.save = jags.param.lipCancerNew, n.chains = 2, n.iter = 15000,
n.burnin = 7500,n.thin=1,model.file = jags.mod.lipCancerNew, DIC=FALSE)



```
```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jagsfitlipCancerNew.mcmc <- as.mcmc(jags.mod.fit.lipCancerNew )
MCMCtrace(jagsfitlipCancerNew.mcmc ,type = 'trace',ind = TRUE, pdf = FALSE)

```


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

gelman.diag(jagsfitlipCancer.mcmc)


```
As we can see everything still converges so we will now extract the new RR posterior values and map the two posterior RRs we have

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

##extract posterior RR
posRRNew <- substr(rownames(jags.mod.fit.lipCancerNew$BUGSoutput$summary),1,2)=='RR'

RRRegionsNew = jags.mod.fit.lipCancerNew$BUGSoutput$summary[posRRNew,1]

ScotlandDF$RRNew = RRRegionsNew

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat = ScotlandDF$RR # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland SMR by region")

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat = ScotlandDF$RRNew # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland SMR by region")

```

As we can see from both maps there seems to be higher risk in some of the regions within the latitudes of 57 and 58 degrees from the new updated posteriors.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}


jags.mod.fit.mcmc <- as.mcmc(jags.mod.fit.lipCancer)
jags.mod.fit.mcmcNew <- as.mcmc(jags.mod.fit.lipCancerNew)


densityplot(jags.mod.fit.mcmc[, "beta0"])
densityplot(jags.mod.fit.mcmcNew[, "beta0"])

```
Plotting $\beta0$ density plots to better identify any patterns changes in the summaries we can see that in the updated model the bell curve is not as flat as it reaches a maximum density of close to 5 while the previous model didn't reach density 4 at its peak of the bell curve, however that means that the new bell curve is narrower. 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}


densityplot(jags.mod.fit.mcmc[, "tau"])
densityplot(jags.mod.fit.mcmcNew[, "tau"])

```
Again plotting $\tau$ density plots to better identify any patterns changes in the summaries we can see that the opposite has occurred from $\beta0$ as the curve is more stretched and more skewed to the right than the previous model.


## Question 2 a)

<!-- To main steps of fitting a Bayesian model using R2jags are the -->
<!-- following. 1. Define the JAGS model (as a function). 2. Prepare the -->
<!-- data, which should be passed on as a list. Note that this list should -->
<!-- include other known variables not in the data, like the length N of the -->
<!-- data vector, when this N is used in the JAGS model definition (e.g. in a -->
<!-- for loop). 3. Identify the parameters whose posterior distributions are -->
<!-- of interest. Here a vector of the parameter names is needed. 4. Define -->
<!-- starting values for JAGS. We need starting values for all the stochastic -->
<!-- nodes (unknown parameters, missing data) in each chain. Initial values -->
<!-- should be passed on as a list. Note that JAGS runs without user-defined -->
<!-- starting values, in which case it generates its own initial values. But -->
<!-- this is not recommended, especially for complex models. 5. Fit the model -->
<!-- in JAGS. The most important arguments of the jags function includes: -->
<!-- data (data list), inits (list of initial values), parameters.to.save -->
<!-- (parameters of interest), n.chains (number of MC chains - each gives a -->
<!-- sequence of simulated values), n.iter (number of iterations in the -->
<!-- sampling process), n.burnin (number of initial samples to discard), -->
<!-- model.file (the JAGS model definition). 6. Convergence diagnostics. -->
<!-- Update if convergence hasn't reached. (See later). 7. Once we are -->
<!-- satisfied that the chains have converged, we can start the statistical -->
<!-- inference. E.g. we can plot the posterior density, get point estimates -->
<!-- and interval estimates by extracting the posterior mean and credible -->
<!-- intervals. -->

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jags.mod.infantDeath <- function(){
for(i in 1:12){
r[i] ~ dbin(theta[i],n[i])
logit(theta[i]) <- logit.theta[i]
logit.theta[i] ~ dnorm(mu, tau)
}
mu ~ dnorm(0,0.001)
tau <- 1/sigma^2
sigma ~ dunif(0,100)
}

# Parameters to monitor
jags.param.infantDeath <- c("mu", "sigma", "theta", "tau")

## initial values
inits1 <- list('mu' = 1, 'sigma' = 1, 'theta'=rep(1,12))
inits2 <- list('mu'= 0,'sigma'=50,'theta'= rep(20,12))

jags.inits = list(inits1,inits2)

## data

jags.data.infantDeath <- list(r = surgicalDF$r, n = surgicalDF$n)

## fit the jags model

jags.mod.fit.infantDeath <- jags(data = jags.data.infantDeath, #inits = jags.inits,
parameters.to.save = jags.param.infantDeath, n.chains = 2, n.iter = 10000,
n.burnin = 5000,n.thin=1,model.file = jags.mod.infantDeath, DIC=FALSE)


```
Now that we have the fitted the model we will observe the plots to check for possible convergence during the recorded 5000 iterations.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jagsfitinfantDeath.mcmc <- as.mcmc(jags.mod.fit.infantDeath )
MCMCtrace(jagsfitinfantDeath.mcmc ,type = 'trace',ind = TRUE, pdf = FALSE)


```

As we can see from the plots, there seems to be no visible patterns on the chains and all parameters have stable oscillation around a common value. We can also observe that the chains seem to be well mixed with each other as it is difficult to distinguish between chains.

However visualization alone is not enough to fully access the convergence so we will use the Gelman-Rubin diagnostics to check for convergence.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

gelman.diag(jagsfitinfantDeath.mcmc)

```
As we can see from the Upper Confidence Interval, the values of all parameters are approximately 1, indicating that the parameters have indeed converged.


### Question 2 b)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

jags.mod.infantDeath <- function(){
for(i in 1:12){
r[i] ~ dbin(theta[i],n[i])
rpred[i] ~ dbin(theta[i],n[i])

logit(theta[i]) <- logit.theta[i]
logit.theta[i] ~ dnorm(mu, tau)

#mid p-value
p[i] =ifelse(rpred[i] > r[i], 1, ifelse(rpred[i] == r[i], 0.5, 0))
 
 

}
mu ~ dnorm(0,0.001)
tau <- 1/sigma^2
sigma ~ dunif(0,100)


}

# Parameters to monitor
jags.param.infantDeath <- c("mu", "sigma", "theta", "tau", "p")

## initial values
inits1 <- list('tau' = 100, 'sigma' = 20, 'theta'=c(10,-5,-25))
inits2 <- list('tau'=100000,'sigma'=-100,'theta'=c(-100,100,500))

jags.inits = list(inits1,inits2)

## data

jags.data.infantDeath <- list(r = surgicalDF$r, n = surgicalDF$n)

## fit the jags model

jags.mod.fit.infantDeath <- jags(data = jags.data.infantDeath, #inits = jags.inits,
parameters.to.save = jags.param.infantDeath, n.chains = 2, n.iter = 10000,
n.burnin = 5000,n.thin=1,model.file = jags.mod.infantDeath, DIC=FALSE)


```
```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

mid_p_summary <- jags.mod.fit.infantDeath$BUGSoutput$summary[grep("p\\[\\d+\\]", rownames(jags.mod.fit.infantDeath$BUGSoutput$summary)), ]

mid_p_summary_means <- subset(mid_p_summary, select = "mean")

# Plot kernel density of mid p-values
plot(density(mid_p_summary_means), main = "Kernel Density Plot of Mid P-values", xlab = "Mid P-value")

```


### Question 2 c)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}


jags.mod.infantDeathUniform <- function(){
  
for(i in 1:12){

    theta[i] ~ dunif(0, 1) ## new theta definition
    r[i] ~ dbin(theta[i],n[i])
    rpred[i] ~ dbin(theta[i],n[i])
    
    p[i] =ifelse(rpred[i] > r[i], 1, ifelse(rpred[i] == r[i], 0.5, 0))
}
  
mu ~ dnorm(0,0.001)
tau <- 1/sigma^2
sigma ~ dunif(0,100)
}

# Parameters to monitor
jags.param.infantDeathUniform <- c("mu", "sigma", "theta", "tau", "p")

## initial values
inits1 <- list('mu' = 1, 'sigma' = 1, 'theta'=rep(1,12))
inits2 <- list('mu'= 0,'sigma'=50,'theta'= rep(20,12))

jags.inits = list(inits1,inits2)

## data

jags.data.infantDeathUniform <- list(r = surgicalDF$r, n = surgicalDF$n)

## fit the jags model

jags.mod.fit.infantDeathUniform <- jags(data = jags.data.infantDeathUniform, #inits = jags.inits,
parameters.to.save = jags.param.infantDeathUniform, n.chains = 2, n.iter = 10000,
n.burnin = 5000,n.thin=1,model.file = jags.mod.infantDeathUniform, DIC=FALSE)



```

After having checked for convergence we will now use box plots to compare the theta mortality rates from the first and the last model.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Extract posterior samples of theta
theta.samples <- jags.mod.fit.infantDeath$BUGSoutput$sims.list$theta
theta.samplesUniform <- jags.mod.fit.infantDeathUniform$BUGSoutput$sims.list$theta

# Create boxplot
boxplot(theta.samples, xlab = "Hospitals", ylab = "Mortality Rate", main = "Posterior Distribution of Theta for the 1st model")

# Create boxplot
boxplot(theta.samplesUniform, xlab = "Hospitals", ylab = "Mortality Rate", main = "Posterior Distribution of Theta For uniform dist model")


```
From the 2 box plots it seems that the changes are mainly in increasing the overall mortality rate theta in the last model as the trends remain identical but all values seem to have a sightly constant increase in value. This means that our new independent fixed effect model has a very similar effect captured compared to the previous mortality rate effect.

### Question 2 d)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}


jags.mod.fit.infantDeath

```

## Question B - classification

### Question B.1

Group 0 is characterised for always having x1 \> -1,65 and x2 \< 2,05
with the majority of the occurrences of group 0 having

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## I am just going to split the dataframe in two so its easier to show an histogram of each variable for both groups

group0 = subset(classificationDF, Group == 0)
group1 = subset(classificationDF, Group == 1)

##removing the id variable from both of these new data frames so they are not included in the histograms
group0 = group0 %>% select(-(1))
group1 = group1 %>% select(-(1))


plot_histogram(group0)


```

As we can see from the histograms from group 0 it seems that X2 seems to
follow a normal distribution that is very slightly skewed to the left
and has mean -0,5. X1 is clearly skewed to the right possibly following
a normal distribution and again mean equal to -0,5.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

plot_histogram(group1)


```

Group 1 histograms reveal that X2 is normally distributed with mean
equal to 0,5 and sightly skewed to the left. The variable X1 also seems
to follow a normal distribution that has a very prolonged skewness to
the left and mean 0,5.0

### Question B.2

Both LDA and QDA assume normality which seems to hold from what we can see
from the histograms on question B.1

Starting with Linear discriminant analysis (LDA) we can easily see from
the initial graph that we can see that the any sensible decision
boundary is highly non-linear. As analysed from histograms this data
violates the initial assumption of homogeneity of variance
(homoscedasticity). The non-linearity of the data would also lead to LDA
having terrible performance. As such we can conclude that LDA is not
suitable for this data.

Secondly, quadratic discriminant analysis (QDA) follow the assumption of
normality of each of the variables which does seem to hold even with how
highly non-linear the data is. However judging from the plot with the
observations, it does seem that QDA does not have enough flexibility to
account for all the variability in what is the theoretical Bayes
decision boundary due to the highly non-linear nature of the data. As such we can conclude that QDA is not suitable for this data.

Thirdly, K-nearest neighbour classification (KNN) has no assumptions
made about the shape of the decision, as such as we choose a adequate
level of smoothness to prevent overfitting KNN classification is an
adequate method for the data.

Support Vector Machines (SVM) are most commonly used for binary
classification, which holds true for our data. The high non-linear
decision boundary however requires a careful selection of a non linear
kernel function to ensure that the boundary becomes non-linear when
converted back to the regular space. As such I deem SVM an appropriate
method to classify the data.

Finally, Random forest also do not make any assumptions about the underlying distribution present on the data set. A random forest would also have the added bonus of letting us understand which of the variables X1 or X2 has the biggest impact on the classification of data points which could be useful for future applications of this model. As such I deem Random forest an appropriate
method to classify the data.


### Question B.3

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# make this example reproducible
set.seed(26041999)
# use 80% of dataset as training set and 20% as test set
sample = sample.split(classificationDF$Group, SplitRatio = 0.8)
## note that the column selected above can be any column.
train = classificationDF[sample, ]
test = classificationDF[!sample, ]

# split the data using the indices returned by the createDataPartition function
X_train = train[, c("X1", "X2")]
y_train = train$Group
X_test = test[, c("X1", "X2")]
y_test = test$Group

# check the dimensions 
dim(train)
dim(test)


```

### Question B.4

The 3 method I will be using are Random forest, KNN and SVM as deemed to be the
most appropriate methods for this data.

#### Random Forest


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, output=FALSE}

trainNum = train
trainNum$Group = as.factor(trainNum$Group)

trainNum = trainNum %>% select(-(1))

testNum = test
testNum$Group = as.factor(testNum$Group)

```

For random Forest we will tune the number of variables randomly split for each of the random samples, as such we will try if only 1 split variable or both variables give a better accuracy

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Random Search for best number of variables
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")

rf_random <- train(Group~., data=trainNum, method="rf", metric="Accuracy", tuneLength=15, trControl=control)
print(rf_random)
plot(rf_random)

```
As we can see the accuracy is almost identical with a difference of less that 0,1% and is actually variable depending on the randomly selected values during the training process due to the nature of the random search as such I will try both mtry = 1  and mtry = 2.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Train the model
rf_model <- randomForest(Group ~ ., data=trainNum, 
                         tuneGrid=data.frame(mtry=1))

# Make predictions on the test data
predictions <- predict(rf_model, testNum)

confusionMatrix(as.factor(predictions), as.factor(y_test))


```
```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Train the model
rf_model <- randomForest(Group~., data=trainNum, 
                         tuneGrid=data.frame(mtry=2))


# Make predictions on the test data
predictions <- predict(rf_model, testNum)

## Manually making the cut at 0,5 of predicition for the groups
#predictions  <- ifelse(predictions >= 0.5, 1, 0)

confusionMatrix(as.factor(predictions), as.factor(y_test))


```

As we can see from the summary tables both fine tuned values lead to approximately the same result of 82% accuracy which is actually lower than the random search was predicting.

#### KNN - K-nearest neighbour

First we make an graph displaying the accuracy for each k from 1 to 30 to see which K is the most appropriate by giving us the best accuracy.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Define range of K values to test
k_range <- 1:30

# Empty list to store validation accuracy scores
accuracy_scores <- c()

# Train and evaluate KNN model for each value of K
for (k in k_range) {
  knn <- knn(X_train, X_test, y_train, k = k)
  accuracy_scores <- c(accuracy_scores, sum(knn == y_test)/length(y_test))
}

# Plot the elbow curve
df <- data.frame(k = k_range, accuracy = accuracy_scores)
ggplot(df, aes(x = k, y = accuracy)) +
  geom_line() +
  geom_point() +
  xlab("K") +
  ylab("Validation Accuracy") +
  ggtitle("Elbow Curve for KNN")

```

As we can see from the graph, k= 9 is the one that give us the most accuracy and most other values have not been as highly accurate.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# fit the model
fit = knn(train=X_train, test=X_test, cl=y_train,k=9)
# fit = knn(xTrain,xTest,yTrain,k=3)

# produce the confusion matrix
# when numbers are used as class labels we might need
confusion = confusionMatrix(as.factor(fit),as.factor(y_test))

confusion

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE}

test$predicted_group = fit
test$correct_prediction = test$predicted_group == test$Group

# plot the data points
p = ggplot() + 
  geom_point(data = train, aes(x = X1, y = X2, color = "Training"), size = 3) +
  geom_point(data = test, aes(x = X1, y = X2, color = correct_prediction, shape = factor(correct_prediction)), size = 3) +
  scale_color_manual(values = c("red", "blue", "green"), labels = c("Incorrect", "Correct", "Training")) +
  scale_shape_manual(values = c(1, 16)) +
  labs(x = "X1", y = "X2", color = "", shape = "") +
  ggtitle("KNN Classification Results")

# display the plot
print(p)

```

#### SVM - Support Vector Machines

Since we don't have a linear kernel due to the nature of the data we have a choice of using either a polynomial kernel or a radial kernel, however there is no simple test to differentiate which kernel function has the best performance for this data, as such I would be fine tuning both kernel functions and then see which kernel function has the best performance.

However since the polynomial fine tuning does not converge I will use the non tuned kernels to show that the radial kernel is better suited to the data.

Startin with the polynomial kernel

```{r, warning=FALSE}

mdlSVMPoly = svm(Group ~ .,
                 data = train,
                 type = 'C-classification',
                 kernel = 'polynomial',
                 #cost = svmPolynoml_tuned$bestTune$C,
                 #sigma = svmPolynoml_tuned$bestTune$sigma
                 )

# Test model on testing data
yTestPredPoly = predict(mdlSVMPoly, newdata=test)
# yTestPred = mdl %>% predict(xTest) 
confusionMatrix(as.factor(yTestPredPoly), as.factor(y_test)) # predicted/true


```
As we can see this kernel function only has an average accuracy of 73% which is significanly lower than the other methods.

```{r, warning=FALSE}

mdlSVM = svm(Group ~ .,
                 data = train,
                 type = 'C-classification',
                 kernel = 'radial')

# Test model on testing data
yTestPred = predict(mdlSVM, newdata=test)
# yTestPred = mdl %>% predict(xTest) 
confusionMatrix(as.factor(yTestPred), as.factor(y_test)) # predicted/true


```
As we can see, the Radial kernel function even when not fine tuned has a significantly better accuracy than the polynomial kernel by almost 10%, such gap would have not been narrowed by the fine tuning of the parameters alone and as such a Radial kernel function is the most appropriate for this data, as such we will be fine tuning the hyper parameters of the model.

Firstly I will start by fine tuning the common constant C and the sigma parameter which is specific to the radial kernel SVM

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

fitControl = trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)

param_grid <- expand.grid(C = seq(0.1, 10, length = 10),
                          sigma = seq(0.1, 1, length = 10))


svmRadial_tuned = train(Group ~ .,
                 data = trainNum,
                 method = 'svmRadial',
                 trControl = fitControl,
                 tuneGrid = param_grid)

svmRadial_tuned

```

This is the non-converging polynomial SVM where I would fine tuning the common constant C and the specific fine tuning parameters from the polynomial function, degree and scale.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE}



# Set up the tuning grid
tuningGrid <- expand.grid(degree = c(2, 3, 4),
                          C = 10^(0:2),
                           scale = c(0.1, 1, 10))

# Set up the control parameters
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           classProbs = TRUE)
## I had to convert the variable names from scientific notation to decimal because it is the only way for this to work for this kernel
 trainNumTest = trainNum

 levels(trainNumTest$Group) <- make.names(levels(trainNumTest$Group))


# Train the SVM with polynomial kernel
svmPoly_tuned <- train(Group ~ .,
                       data = trainNumTest,
                       method = "svmPoly",
                       tuneGrid = tuningGrid,
                       trControl = fitControl,
                       minstep = 1e-08)


# Print the results
svmPoly_tuned


```

Now that we have the fine tuned parameters I will be using these to fit a model for the Radial kernel.

```{r, warning=FALSE}

mdlSVM = svm(Group ~ .,
                 data = train,
                 type = 'C-classification',
                 kernel = 'radial',
                 cost = svmRadial_tuned$bestTune$C,
                 sigma = svmRadial_tuned$bestTune$sigma)

# Test model on testing data
yTestPred = predict(mdlSVM, newdata=test)
# yTestPred = mdl %>% predict(xTest) 
confusionMatrix(as.factor(yTestPred), as.factor(y_test)) # predicted/true


```

As we can see from the fine tuned model summary, the usage of C and sigma managed to improve the model by approximately 0,5% making this model slightly better than the prevviously untuned model.


### Question B.5

In our data we have 475 observations belonging to the group 0 and the remaining 525 belonging to group 1, since there is only a small difference in number of observations between both groups, accuracy will be a sensible metric to compare all 3 different methods.

From the 3 final fine tuned models we can see that the Radial kernel SVM has an accuracy of 83%, the KNN with k=9 has an accuracy of 85% and the random forest has an accuracy of 82%, as such with overall accuracy in mind we can determine that KNN with 9 clusters is the better predictive model.

This also holds true if we were to compare the specificity of the 3 models as KNN has the highest value of 83,81% compared to the 80,00% from random forest and 78,10% from SVM.

However for sensitivity the SVM has 88,42% which beats both the SVM 86,32% and the random forest 84,32%.

To conclude unless there was a specific need to focus on the specificity of the model which is not given for this dataset the best method for classification is the K-nearest neighbour classification.

