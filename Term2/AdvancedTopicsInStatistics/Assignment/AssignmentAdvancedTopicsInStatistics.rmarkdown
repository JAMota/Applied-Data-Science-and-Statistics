---
title: "AdvancedStatsAssignment"
format: pdf
editor: visual
---

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, output=FALSE}

library(C50)
library(caret) # function for creating train/test datasets, and confusion matrices
library(class) # this has the knn function
library(ggplot2)
library(tidyverse)
library(tidyr)
library(DataExplorer)
library(caTools)
library(kernlab)
library(e1071)


library(RColorBrewer)
library(sf)
library(rgdal)

#install.packages("sf")

library(reshape2)

library(R2jags); library(MCMCvis); library(coda); library(lattice)

classificationDF = read_csv("Classification.csv")

ScotlandDF = read_csv("Scotland.csv")


```


## Question 1


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE}

source("ScotlandMap.R") # need to read in the Scotland Map function
testdat <- runif(56) # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland random numbers")

```

## Question 1 a)


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

ScotlandDF$SMR = ScotlandDF$Observed /ScotlandDF$Expected

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

hist(ScotlandDF$SMR, breaks = 20, col = "lightblue",
     main = "Distribution of Standard Mortality Ratios",
     xlab = "SMR")

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

source("ScotlandMap.R") # need to read in the ScotlandMap function
testdat <- ScotlandDF$SMR # generate random numbers to use as observations
ScotlandMap(testdat,figtitle="Scotland random numbers")


```


## Question 1 b)

\begin{align*}
Obs_i \sim \text{Pois}(\mu_i) \
\log(\mu_i) = \log(\text{Exp}_i) + \beta_0 + \theta_i \
\text{RR}_i = \exp(\beta_0) \exp(\theta_i)
\end{align*}

\begin{align*}
Obs_i \sim \text{Pois}(\mu_i) \[1ex]
\log(\mu_i) = \log(\text{Exp}_i) + \beta_0 + \theta_i \[1ex]
\text{RR}_i = \exp(\beta_0) \exp(\theta_i)
\end{align*}

Beta0 is the intersect 



```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}




```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}




```


## Question 2 - classification

## Question 2.1

Group 0 is characterised for always having x1 \> -1,65 and x2 \< 2,05
with the majority of the occurences of group 0 having


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## I am just going to split the dataframe in two so its easier to show an histogram of each variable for both groups

group0 <- subset(classificationDF, Group == 0)
group1 <- subset(classificationDF, Group == 1)

##removing the id variable from both of these new data frames so they are not included in the histograms
group0 = group0 %>% select(-(1))
group1 = group1 %>% select(-(1))


plot_histogram(group0)


```


As we can see from the histograms from group 0 it seems that X2 seems to
follow a normal distribution that is very slightly skewed to the left
and has mean -0,5. X1 is clearly skewed to the right possibly following
a normal distribution and again mean equal to -0,5.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

plot_histogram(group1)


```


Group 1 histograms reveal that X2 is normally distributed with mean
equal to 0,5 and sightly skewed to the left. The variable X1 also seems
to follow a normal distribution that has a very prolonged skewness to
the left and mean 0,5.0

## Question 2.2

Both LDA and QDA assume normality which is true from what we can see
from the histograms on 2.1

Starting with Linear discriminant analysis (LDA) we can easily see from
the initial graph that we can see that the any sensible decision
boundary is highly non-linear. As analysed from histograms this data
violates the initial assumption of homogeneity of variance
(homoscedasticity). The non-linearity of the data would also lead to LDA
having terrible performance. As such we can conclude that LDA is not
suitable for this data.

Secondly, quadratic discriminant analysis (QDA) follow the assumption of
normality of each of the variables which does seem to hold even with how
highly non-linear the data is. However judging from the plot with the
observations, it does seem that QDA might not have enough flexibility to
account for all the variability in what is the theoretical Bayes
decision boundary. As such QDA could be appropriate for this data but
not the best performing classification method for such data.

Thirdly, K-nearest neighbour classification (KNN) has no assumptions
made about the shape of the decision, as such as we choose a adequate
level of smoothness to prevent overfitting KNN classification is an
adequate method for the data.

Support Vector Machines (SVM) are most commonly used for binary
classification, which holds true for our data. The high non-linear
decision boundary however requires a careful selection of a non linear
kernel function to ensure that the boundary becomes non-linear when
converted back to the regular space. As such I deem SVM an appropriate
method to classify the data.

Finally, Random forest works by constructing multiple decision trees
randomly selecting a portion of the variables and de-correlate them
using multiple bootstrapped sets. Since our data has only 2 variables
there isn't enough variables to overcome overfitting. To conclude,
Random Forest classification is not very appropriate to this type of
data.

## Question 2.3


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# make this example reproducible
set.seed(26041999)
# use 80% of dataset as training set and 20% as test set
sample <- sample.split(classificationDF$Group, SplitRatio = 0.8)
## note that the column selected above can be any column.
train <- classificationDF[sample, ]
test <- classificationDF[!sample, ]

# split the data using the indices returned by the createDataPartition function
X_train <- train[, c("X1", "X2")]
y_train <- train$Group
X_test <- test[, c("X1", "X2")]
y_test <- test$Group

# check the dimensions 
dim(train)
dim(test)


```


## Question 2.4

The 3 method I will be using are QDA, KNN and SVM as deemed to be the
most appropriate methods for this data.

## QDA - quadratic discriminant analysis


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```


## KNN - K-nearest neighbour


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# fit the model
fit = knn(train=X_train, test=X_test, cl=y_train,k=3)
# fit = knn(xTrain,xTest,yTrain,k=3)

# produce the confusion matrix
# when numbers are used as class labels we might need
confusion = confusionMatrix(as.factor(fit),as.factor(y_test))

confusion

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

test$predicted_group <- fit
test$correct_prediction <- test$predicted_group == test$Group

# plot the data points
p <- ggplot() + 
  geom_point(data = train, aes(x = X1, y = X2, color = "Training"), size = 3) +
  geom_point(data = test, aes(x = X1, y = X2, color = correct_prediction, shape = factor(correct_prediction)), size = 3) +
  scale_color_manual(values = c("red", "blue", "green"), labels = c("Incorrect", "Correct", "Training")) +
  scale_shape_manual(values = c(1, 16)) +
  labs(x = "X1", y = "X2", color = "", shape = "") +
  ggtitle("KNN Classification Results")

# display the plot
print(p)

```


## SVM - Support Vector Machines


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## checking the model performance with multiple difference C constants
mdl <- train(x=X_train,y=y_train, method = "svmLinear",
trControl = trainControl("cv", number = 5),
tuneGrid = expand.grid(C = seq(0, 2, length = 20)))
# Plot model accuracy vs different values of Cost
plot(mdl)
# Print the best tuning parameter C that maximises model accuracy
mdl$bestTune



```


#TODO talk about how there is a balance on how the number of miss
classified


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Test model on testing data
yTestPred <- predict(mdl, newdata=X_test)
confusionMatrix(yTestPred, y_test) # predicted/true

```


## quick debug que funciona CARALHOOOOOOOOOOOOOOOO

## TODO this is using radial but I should have one with polynimial as well to show that polinomial is better


```{r, warning=FALSE}

## https://github.com/MatheusSchaly/Online-Courses/blob/master/Machine_Learning_A-Z_Hands-On_Python_%26_R_In_Data_Science/2_Classification/R/4_Kernel_SVM.R
## to add visualization

mdl <- train(x=X_train,y=y_train, method='svmRadial') 
print(mdl)

mdl = svm(Group ~ .,
                 data = train,
                 type = 'C-classification',
                 kernel = 'radial')

# Test model on testing data
yTestPred = predict(mdl, newdata=test)
# yTestPred <- mdl %>% predict(xTest) 
confusionMatrix(as.factor(yTestPred), as.factor(y_test)) # predicted/true


```

