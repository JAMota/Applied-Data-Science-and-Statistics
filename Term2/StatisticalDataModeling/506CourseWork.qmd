---
title: "506CourseWork"
format:
  pdf:
    toc: true
editor: visual
---

## For the love of god don't forget

Do not display too much raw R output (e.g. don't display the full output
of 'summary(model)'), but edit this down to the essentials. Ensure to
include justification for each step of your analyses, providing comments
alongside your R code to explain what you are doing and add appropriate
titles and labelled axes to your plots.

## Question 1

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE}

load("C:/AppliedDataScienceAndStatistics/Applied-Data-Science-and-Statistics/Term2/StatisticalDataModeling/datasets_exercises_Assessment.RData")

## https://tilburgsciencehub.com/building-blocks/collaborate-and-share-your-work/write-your-paper/amsmath-latex-cheatsheet/ // cheat worksheet

library(ggplot2)
library(dplyr)
require(MASS)


```

We have the model:

$Yi$ ~ $N(\frac{\theta1xi}{\theta2 + xi}$, $\sigma^2)$

## Question 1 a)

Due to the visible non-linearity of the model, we would be required to
significantly transform our data to get a linear model that would have
an acceptable fit of the data. We can also see that the response data
seems to be only positive while a normal distribution goes from
]-$\infty$,$\infty$[ . Such arbitrary transformation increases the
complexity of the model, making it less interpretable and not respect
the nature of the data.

Linear regression models are based on the assumption that the
relationship between the independent and dependent variables is linear.
If the relationship between the variables is non-linear, a linear
regression model may not be appropriate to use. In such cases,
transforming the data to make the relationship linear may not result in
an accurate representation of the true relationship, and can lead to
overfitting or underfitting. Additionally, transforming the data can
result in a loss of interpretability of the results, as it can be
difficult to understand the meaning of the transformed variables.

Another issue with using a linear regression model for non-linear data
is that the residuals, which represent the difference between the
observed and predicted values, may not be normally distributed, which is
another assumption of linear regression models. This can lead to biased
or incorrect results.

In conclusion, when the data is non-linear, a linear regression model
may not be the best choice for modelling the relationship between the
variables, and alternative methods need to be considered.

<!-- // make a graph to show the data is not linear -->

<!-- //should I talk about the increase in variance or is it only topic 2? Not really because we don't have enough data at the start of the graph -->

## Question 1 b)

The Yi are independent so the likelihood is a product of the individual
pdfs.


Likelihood of a normal distribution where $L(yi|\mu,\sigma^2)$ = 

= $\Pi^n_{i=1} fX(yi|\mu,\sigma^2)$

= $\Pi^n_{i=1} ((2\pi\sigma^2)^{-\frac{1}{2}}* exp{(-\frac{1}{2} * \frac{(yi - \mu)^2}{\sigma^2}))}$ =

= $(2\pi\sigma^2)^{-\frac{n}{2}} * exp{(-\frac{1}{2\sigma^2}* \sum_{i=1}^{n} (yi-\mu)^2)}$

Plugging in the $\mu$ with the respective $\theta s$ formula and n, we have the likelihood as:

$L(\beta0,\beta1,\sigma^2;x,y)$ = 

= $\prod_{i=1}^{n}$ $p(\beta0,\beta1,\sigma^2;x,y)$ = 

= $(2\pi\sigma^2)^{-\frac{100}{2}} * exp{(-\frac{1}{2\sigma^2}* \sum_{i=1}^{100}(yi-\frac{\theta1xi}{\theta2+xi})^2)}$


The log-likelihood of a normal distribution is:

$l(yi|\mu,\sigma^2)$ =

= $ln(L(yi|\mu,\sigma^2))$ =

= $ln((2\pi\sigma^2)^{-\frac{n}{2}} * exp{(-\frac{1}{2\sigma^2}* \sum_{i=1}^{n} (yi-\mu)^2)})$ =

= $ln((2\pi\sigma^2)^{-\frac{n}{2}}) + ln(exp{(-\frac{1}{2\sigma^2}* \sum_{i=1}^{n} (yi-\mu)^2)})$ =

= $-\frac{n}{2}ln(2\pi\sigma^2) -\frac{1}{2\sigma^2}* \sum_{i=1}^{n} (yi-\mu)^2)$ =

= $-\frac{n}{2}ln(2\pi) -\frac{n}{2}ln(\sigma^2) -\frac{1}{2\sigma^2}* \sum_{i=1}^{n} (yi-\mu)^2)$

Once again plugging in the $\mu$ with the respective $\theta s$ formula and n, we have the log-likelihood as:

$l(\beta0,\beta1,\sigma^2;x,y)$ =

= $-\frac{100}{2}ln(2\pi) -\frac{100}{2}ln(\sigma^2) -\frac{1}{2\sigma^2}* \sum_{i=1}^{100} (yi-\frac{\theta1xi}{\theta2+xi})^2)$ =

= $-50ln(2\pi) -50ln(\sigma^2) -\frac{1}{2\sigma^2}* \sum_{i=1}^{100} (yi-\frac{\theta1xi}{\theta2+xi})^2)$


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, eval=FALSE}
## https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood 
## god bless 
## this one is not bad at explaining https://medium.com/@lorenzojcducv/maximum-likelihood-for-the-normal-distribution-966df16fd031
```

## Question 1 c)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

n = nrow(nlmodel)


### Create a function to evaluate minus the log-likelihood
myLike = function(variables){
  
theta1 = variables[1] #theta1
theta2 = variables[2] #theta2
sigma = variables[3] #sigma

mu = ((theta1*nlmodel$x)/(theta2+nlmodel$x))

# Log-likelihood
result = (-(n/2)*log(2*pi)) -((n/2)*log(sigma^2)) -(1/(2*(sigma^2))) * (sum((nlmodel$y-mu)^2))

# Returning negative log-likelihood
return(-result)
}



```

## Question 1 d)

<!-- From the graph data we can see that as x -> 0 y is approximately to 45  -->
<!-- and mean is approximately 0 since $\theta1*0/\theta2+0$ is approx 0 -->

From the graph data we can clearly see that the deviantion is approximatly 15 from the value scatter which is easier to destinguish and measure between the $[0.5,1]$ interval.
We can observethat when x -> 1 y is approximately 210 and as x -> 0 y is approximately to 50

To determine the thetas we will first see see that when x approximates to zero we can observe that:

$\underset{x->0}{lim} \frac{\theta1 xi}{\theta2 +xi} \implies \frac{1}{\theta2}$ 

As such $ Y$ ~ $N(\frac{\theta1 xi}{\theta2 +xi})$ becomes $ 50$ ~ $\frac{1}{\theta2}$

Solving it for $\theta2$ we get $\theta2 = 1/50 \iff \theta2 = 0.02$

Now that we have theta2 we can use the approximation of x to 1 to determine the value of $\theta1$

$\underset{x->1}{lim} \frac{\theta1 xi}{\theta2 +xi} \implies \frac{\theta1}{\theta2+x}$ 

As such $ Y$ ~ $N(\frac{\theta1 xi}{\theta2 +xi})$ becomes $215$ ~ $\frac{\theta1*1}{\theta2+1}$


Solving it for $\theta1$ we get $\theta1 = 215/1.02 \iff \theta1 \approx 210.7$


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Estimating the MLE
out <- nlm(myLike,
  p = c(210.7,0.02,15), #plugging in the starting values
  hessian = T,
  iterlim = 10000,
  steptol = 1e-10)

# Reporting estimates
variableEstimates = out$estimate
out$estimate

```
## Question 1 e)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Invert the negated Hessian to obtain the Observed Information Matrix
OIM <- solve(out$hessian)

# The diagonal entries are the variances of beta0 and beta1 respectively so # obtain them
VarianceBeta <- diag(OIM)

# and then square root them to obtain standard errors
stand_error <- sqrt(VarianceBeta)

# reporting standard errors
stand_error


```
The formula to calculate a 99% confidence interval is: β ± 2.576 * SE(β)


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Estimating CIs
CIs <- cbind(variableEstimates - 2.576 * stand_error, variableEstimates + 2.576 * stand_error)

# Reporting the CIs
CIs


```


## Question 1 f)

H0 : θ2 = 0,08 vs. H1 : θ2 $\ne$ 0,08

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## Hypothesis thesis without using confidence interval

z_stat <- (variableEstimates[2] - 0.08)/stand_error[2]

# Print the test values
z_stat ## significance tests


```
So now we need to decide if this value of the z-statistic is extreme at the 10% significance level

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

### Note that equivalently we can look at the 95% quantile of N(0,1)
qnorm(0.95,0,1)

qnorm(0.05,0,1)

```
As we can see the value from the z-statistician test is considerably lower than -1,645, meaning that it is an extreme value and therefore rejecting the null hypothesis that $\theta2$ is 0,08. 

## Question 1 g)


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Plotting initial starting guess
xx <- seq(0, 1, len=200)

# Estimating mean relationship (mean mu = )
mu <- (out$estimate[1 ] * xx) /(out$estimate[2] + xx)

# Getting standard Deviation
standardDeviation <- out$estimate[3]


# Getting 95% interval from the quantiles of a Normal distribution
plot(nlmodel$x,nlmodel$y,pch=20, xlab = 'X variable', ylab = 'Y variable')

lines(xx, qnorm(0.025, mean = mu, sd = standardDeviation), col = 'blue', lty = 'dashed')
lines(xx, qnorm(0.975, mean = mu, sd = standardDeviation), col = 'blue', lty = 'dashed')

lines(xx, qnorm(0.5, mean = mu, sd = standardDeviation), col = 'red')



```

From the estimations produced through our model we can see that the current model with a 95% prediction fits the data quite nicely, having only 4 of the 100 observations shortly out of the 95% prediction interval.

However it should be noted that even though the model has a good performance, a normal distribution has the assumption that data can take any value in the real line however this data is bounded between the [0,1] interval.

Considering that the variance increases trough the model it further indicates that a normal distribution should be switched for another distribution that better respects the nature of our data.

## Question 2

Model 1:

$Yi$ ~ $Pois(\lambda i)$

$log(\lambda i) = \beta0 + \beta1 xi$

Model 2:

$Yi$ ~ $N(\mu i,\sigma^2)$

$log(\mu i) = \gamma0 +\gamma1 xi$

## Question 2 a)

As we can from the graph and what we can determine from the nature of
the data represented in such graph the recorded number of AIDS cases is
a count variable and the counts are non-negative integers.

The first model, a Poisson distribution, would be a more appropriate
choice. The Poisson distribution is a discrete distribution that models
count data which respects the nature of the data

The second model, a Normal distribution, would not be the best fit since
its range is from ]-$\infty$,$\infty$[ and expects continuous values,
not respecting the nature of the data.

The log-link function in both models ensures that the predicted values
are always positive. This behaviour is standard for a poisson distribution but is inadequate for a normal distribution.


## Question 2 b)

The Yi are independent so the likelihood is a product of the individual pdfs.

$L(\theta1,theta2,\sigma^2;y,x)$


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
#Fitting in R

model2 = glm(cases ~ date, data = aids, family = poisson(link='log'))

# Summarise the model
summary(model2)


#Fitting model 1
model1 = glm(cases ~ date, data = aids, family = gaussian(link='log'))

# Summarise the model
summary(model1)

## Sorry Matthew, but it seems I have switched the model numbers and 
## I only noticed after finished the exercise

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## We can use this to obtain 95% confidence intervals on our estimated relationship
xx <- seq(83, 94, len=45)

##predict Poisson
predPoisson = predict(model2, newdata = aids, type="link", se.fit = T)


## Mean and Confidence Intervals estimation
muPoisson = exp(predPoisson$fit)
muPoisson_upper = exp(predPoisson$fit+qnorm(1-1.96/2)*predPoisson$se.fit) ##exp(p$fit+qnorm(1-a/2)*p$se.fit)
muPoisson_lower = exp(predPoisson$fit-qnorm(1-1.96/2)*predPoisson$se.fit) ## this is the log ver in that was in cheat, ask if this is the correct way since we have a link function or I am just bad


##predict normal
predNormal = predict(model1, newdata = aids, type="link", se.fit = T)

## Mean and Confidence Intervals estimation
muNormal = exp(predNormal$fit)
muNormal_upper = exp(predNormal$fit+qnorm(1-1.96/2)*predNormal$se.fit) ## p$fit+qnorm(1-a/2)*p$se.fit
muNormal_lower = exp(predNormal$fit-qnorm(1-1.96/2)*predNormal$se.fit) ## p$fit-qnorm(1-a/2)*p$se.fit

##plot the data

plot(aids$date,aids$cases, pch=20, xlab = "date", ylab = "Number of aids cases")

lines(xx, muPoisson, col="blue")
lines(xx,muPoisson_upper, col="blue", lty = 'dashed')
lines(xx,muPoisson_lower, col="blue", lty = 'dashed')

lines(xx,muNormal,col="red")
lines(xx,muNormal_upper,col="red", lty = 'dashed')
lines(xx,muNormal_lower,col="red", lty = 'dashed')

## even if the Confidence Intervals are not exactly clear, it is still correct to make the exp of the mean and CI When we are using a log-link function

```
As we can see from the plot alone, the linear model fits the data better than the poisson model.
The poisson model is clearly not a good model since the big majority of the model is either overestimating or underestimating the data, which is a clear indicator that this model is inadequate.
The Normal distribution, seems to fit somewhat the data due to the data although non-linear not deviating too much from a line but it is still clearly that it is not an adequate model since there is too many points that are under or overestimated, making it also a not very adequate model.

## Question 2 c)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

#Model comparison aka IAC time

#the formula to for the AIC: −2l + 2p

AIC(model1)

AIC(model2)


```
<!-- ## not sure about this one -->
<!-- #logLik(model2) - logLik(model1) -->

As we can see from the Akaike Information Criterion, model 1 as a much lower AIC, meaning model 1 has a much better fit than model 2 since when comparing AICs, the model with the lower values has the better fit


## Question 2 d)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

plot(model1,1)

```

As we can see from the Residual vs fitted model, there seems to be quadratic pattern in how the model overfits and underfits the data in a way the would require more flexibility from a quadratic term.

<!-- //write same for the linear -->


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

plot(model2,1)

```
The Poisson model residuals seem to follow a quadratic function with some slight curves along the quadric function, this mean it required a quadratic and a cubic term for the flexibility to better fit the data.
 
 
## Question 2 e)
 
 As we commented on the previous exercise we will be adding the quadratic and cubic term values directly to the aids so we can use them in our model

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

aids$dataSq = aids$date^2

aids$dataCubic = aids$date^3


``` 
 
```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



#Fitting model 2 improvement
model2Improved = glm(cases ~ date + dataSq + dataCubic, data = aids, family = poisson(link='log'))

summary(model2Improved)

#Fitting model 1 improvement
model1Improved = glm(cases ~ date + dataSq, data = aids, family = gaussian(link='log'))

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## compare the full and reduced models using ANOVA (this order)
anova(model2Improved, model2, test="Chisq")

```
We can see that the p-value is less than 0.05 and therefore we can reject the null hypothesis and
conclude that the model with the quadratic and cubic terms is better than the linear Poisson model statistically so we should choose the improved model.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## compare the full and reduced models using ANOVA (this order)
anova(model1Improved, model1, test="Chisq")

```
We can once again see that the p-value is less than 0.05 and therefore we can reject the null hypothesis and
conclude that the model with the quadratic term is better than the linear Gaussian model statistically so we should choose the improved model.


## Question 2 f)

<!-- model fit based on the deviance arguments means anova testing -->

Firstly I will talk about each model flaws.

Starting from the improved Gaussian model, its main flaw is how a normal distribution simply does not respect the nature of this data, that is count data with only values on the real line for the reasons already mentioned on question 2 a).

The improved Poisson model although respects the nature of the nature, the initial model was underestimating and overestimating too much of the data and required a clear flexibility improvement by adding quadratic and cubic terms, however such performance improvement come at the cost of interpretability as it is not very clear what quadratic and cubic time real represent in this new model.

Now that we have established that both models have their flaws I will begin by comparing both models.

We can extract the deviance from the model and calculate a p-value to check whether the
model fits the data using an LRT:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
# Deviance goodness of fit of model 1 looks OK:
1 - pchisq(model1Improved$deviance, model1Improved$df.residual)

# Deviance goodness of fit of model 1 looks OK:
1 - pchisq(model2Improved$deviance, model2Improved$df.residual)
```
The p-value for both models are smaller than 0.05, so it means that both models although already improved still aren't a good fit for the data.

<!-- // DRAW THE PLOTS of these 2 omdels -->

<!-- ```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)} -->

<!-- ## We can use this to obtain 95% confidence intervals on our estimated relationship -->
<!-- xx <- seq(83, 94, len=45) -->

<!-- ##predict improved Poisson -->
<!-- predPoissonImproved = predict(model2Improved, newdata = aids, type="link", se.fit = T) -->


<!-- ## Mean and Confidence Intervals estimation -->
<!-- muPoissonImproved = exp(predPoissonImproved$fit) -->
<!-- muPoissonImproved_upper = exp(predPoissonImproved$fit+qnorm(1-1.96/2)*predPoissonImproved$se.fit)  -->
<!-- muPoissonImproved_lower = exp(predPoissonImproved$fit-qnorm(1-1.96/2)*predPoissonImproved$se.fit)  -->

<!-- ##predict Normal improved -->
<!-- predNormalImproved = predict(model1Improved, newdata = aids, type="link", se.fit = T) -->

<!-- ## Mean and Confidence Intervals estimation -->
<!-- muNormalImproved = exp(predNormal$fit) -->
<!-- muNormalImproved_upper = exp(predNormalImproved$fit+qnorm(1-1.96/2)*predNormalImproved$se.fit) -->
<!-- muNormalImproved_lower = exp(predNormalImproved$fit-qnorm(1-1.96/2)*predNormalImproved$se.fit) -->

<!-- ##plot the data -->

<!-- plot(aids$date,aids$cases, pch=20, xlab = "date", ylab = "Number of aids cases") -->

<!-- lines(xx, muPoissonImproved, col="blue") -->
<!-- lines(xx,muPoissonImproved_upper, col="blue", lty = 'dashed') -->
<!-- lines(xx,muPoissonImproved_lower, col="blue", lty = 'dashed') -->

<!-- lines(xx,muNormalImproved,col="red") -->
<!-- lines(xx,muNormalImproved_upper,col="red", lty = 'dashed') -->
<!-- lines(xx,muNormalImproved_lower,col="red", lty = 'dashed') -->

<!-- ``` -->


Comparing both models using AIC we can see that:
```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
AIC(model1Improved)
AIC(model2Improved)
```
Both models have a relatively close AIC result. 

Leaving us to choose neither of the models as good fit for the data, although the improved Poisson would still be preferred to the improved Gaussian since it is the only one of the improved models respecting the nature of the data.



## Question 2 g)


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Fit the model 
##model2Improved = glm(cases ~ date + dataSq + dataCubic, data = aids, family = poisson(link='log'))
modelNegativeBinom <- glm.nb(cases ~ (date + dataSq + dataCubic ), data = aids)

# Model summary
summary(modelNegativeBinom)

```
<!-- ## Negative Bonomial is a Quase Poisson that is already on the log scale -->

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Fit the model 
modelNegativeBinom <- glm.nb(cases ~ (date + dataSq + dataCubic ), data = aids)

# Model summary
summary(modelNegativeBinom)

```

Now that we have extended the improved Poisson into a negative Binomial, we will compare this new model against the previously improved Poisson model and the improved Gaussian model.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
# Deviance goodness of fit of the negative model looks OK:
1 - pchisq(modelNegativeBinom$deviance, modelNegativeBinom$df.residual)

# Deviance goodness of fit of improved model 2 looks OK:
1 - pchisq(model2Improved$deviance, model2Improved$df.residual)

# Deviance goodness of fit of improved model 1 looks OK:
1 - pchisq(model1Improved$deviance, model1Improved$df.residual)
```
As we can see, the negative binomial model is the only model with a p-value is larger than 0.05, meaning that only negative binomial is considered a good fit to the data. 

Lastly from the Akaike Information Criterion test we can see that the Negative Binomial has the best goodness of fit per penalised complexity meaning it simply performs better than the other 2 models.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

AIC(modelNegativeBinom)

AIC(model1Improved)
AIC(model2Improved)

```

To conclude the negative binomial model not only is preferable to both the improved poisson and improved gaussian model, it is also a good fit for the data.




