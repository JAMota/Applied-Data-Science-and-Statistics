---
title: "506CourseWork"
format:
  pdf:
    toc: true
editor: visual
---

## For the love of god don't forget

Do not display too much raw R output (e.g. don't display the full output
of 'summary(model)'), but edit this down to the essentials. Ensure to
include justification for each step of your analyses, providing comments
alongside your R code to explain what you are doing and add appropriate
titles and labelled axes to your plots.

## Question 1

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE}

load("C:/AppliedDataScienceAndStatistics/Applied-Data-Science-and-Statistics/Term2/StatisticalDataModeling/datasets_exercises_Assessment.RData")

## https://tilburgsciencehub.com/building-blocks/collaborate-and-share-your-work/write-your-paper/amsmath-latex-cheatsheet/ // cheat worksheet

library(ggplot2)


```

We have the model:

$Yi$ ~ $N(\frac{\theta1xi}{\theta2 + xi}$, $\sigma^2)$

## Question 1 a)

Due to the visible non-linearity of the model, we would be required to
significantly transform our data to get a linear model that would have
an acceptable fit of the data. We can also see that the response data
seems to be only positive while a normal distribution goes from
]-$\infty$,$\infty$[ . Such arbitrary transformation increases the
complexity of the model, making it less interpretable and not respect
the nature of the data.

Linear regression models are based on the assumption that the
relationship between the independent and dependent variables is linear.
If the relationship between the variables is non-linear, a linear
regression model may not be appropriate to use. In such cases,
transforming the data to make the relationship linear may not result in
an accurate representation of the true relationship, and can lead to
overfitting or underfitting. Additionally, transforming the data can
result in a loss of interpretability of the results, as it can be
difficult to understand the meaning of the transformed variables.

Another issue with using a linear regression model for non-linear data
is that the residuals, which represent the difference between the
observed and predicted values, may not be normally distributed, which is
another assumption of linear regression models. This can lead to biased
or incorrect results.

In conclusion, when the data is non-linear, a linear regression model
may not be the best choice for modelling the relationship between the
variables, and alternative methods need to be considered.

// make a graph to show the data is not linear

<!-- //should I talk about the increase in variance or is it only topic 2? Not really because we don't have enough data at the start of the graph -->

## Question 1 b)

The Yi are independent so the likelihood is a product of the individual
pdfs.

## TODO GET BETTER WORDING FOR THE REPLACE PART 


Likelihood of a normal distribution where $L(yi|\mu,\sigma^2)$ = 

= $\Pi^n_{i=1} fX(yi|\mu,\sigma^2)$

= $\Pi^n_{i=1} (2\pi\sigma^2)^{-\frac{1}{2}}* exp{(-\frac{1}{2} * \frac{(yi - \mu)^2}{\sigma^2})}$ =

= $(2\pi\sigma^2)^{-\frac{n}{2}} * exp{(-\frac{1}{2\sigma^2}* \sum_{i=1}^{n} (yi-\mu)^2)}$

Replacing the $\mu$ with the respective $\theta s$ and n, we have the likelihood as:

$L(\beta0,\beta1,\sigma^2;x,y)$ = 

= $\prod_{i=1}^{n}$ $p(\beta0,\beta1,\sigma^2;x,y)$ = 

= $(2\pi\sigma^2)^{-\frac{100}{2}} * exp{(-\frac{1}{2\sigma^2}* \sum_{i=1}^{100}(yi-\frac{\theta1xi}{\theta2+xi})^2)}$


The log-likelihood of a normal distribution is:

$l(yi|\mu,\sigma^2)$ =

= $ln(L(yi|\mu,\sigma^2))$ =

= $ln((2\pi\sigma^2)^{-\frac{n}{2}} * exp{(-\frac{1}{2\sigma^2}* \sum_{i=1}^{n} (yi-\mu)^2)})$ =

= $ln((2\pi\sigma^2)^{-\frac{n}{2}}) + ln(exp{(-\frac{1}{2\sigma^2}* \sum_{i=1}^{n} (yi-\mu)^2)})$ =

= $-\frac{n}{2}ln(2\pi\sigma^2) -\frac{1}{2\sigma^2}* \sum_{i=1}^{n} (yi-\mu)^2)$ =

= $-\frac{n}{2}ln(2\pi) -\frac{n}{2}ln(\sigma^2) -\frac{1}{2\sigma^2}* \sum_{i=1}^{n} (yi-\mu)^2)$

Once again replacing the $\mu$ with the respective $\theta s$ and n, we have the log-likelihood as:

$l(\beta0,\beta1,\sigma^2;x,y)$ =

= $-\frac{100}{2}ln(2\pi) -\frac{100}{2}ln(\sigma^2) -\frac{1}{2\sigma^2}* \sum_{i=1}^{100} (yi-\frac{\theta1xi}{\theta2+xi})^2)$ =

= $-50ln(2\pi) -50ln(\sigma^2) -\frac{1}{2\sigma^2}* \sum_{i=1}^{100} (yi-\frac{\theta1xi}{\theta2+xi})^2)$


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, eval=FALSE}
## https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood 
## god bless 
## this one is not bad at explaining https://medium.com/@lorenzojcducv/maximum-likelihood-for-the-normal-distribution-966df16fd031
```

## Question 1 c)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

n = nrow(nlmodel)


### Create a function to evaluate minus the log-likelihood
myLike = function(variables){
  
theta1 = variables[1] #theta1
theta2 = variables[2] #theta2
sigma = variables[3] #sigma

mu = ((theta1*nlmodel$x)/(theta2+nlmodel$x))

# Log-likelihood
result = (-(n/2)*log(2*pi)) -((n/2)*log(sigma^2)) -(1/(2*(sigma^2))) * (sum((nlmodel$y-mu)^2))


# Returning negative log-likelihood
return(-result)
}



```

## Question 1 d)

<!-- From the graph data we can see that as x -> 0 y is approximately to 45  -->
<!-- and mean is approximately 0 since $\theta1*0/\theta2+0$ is approx 0 -->

From the graph data we can clearly see that the deviantion is approximatly 15 from the value scatter which is easier to destinguish and measure between the $[0.5,1]$ interval.
We can observethat when x -> 1 y is approximately 210 and as x -> 0 y is approximately to 50

To determine the thetas we will first see see that when x approximates to zero we can observe that:

$\underset{x->0}{lim} \frac{\theta1 xi}{\theta2 +xi} \implies \frac{1}{\theta2}$ 

As such $ Y$ ~ $N(\frac{\theta1 xi}{\theta2 +xi})$ becomes $ 50$ ~ $\frac{1}{\theta2}$

Solving it for $\theta2$ we get $\theta2 = 1/50 \iff \theta2 = 0.02$

Now that we have theta2 we can use the approximation of x to 1 to determine the value of $\theta1$

$\underset{x->1}{lim} \frac{\theta1 xi}{\theta2 +xi} \implies \frac{\theta1}{\theta2+x}$ 

As such $ Y$ ~ $N(\frac{\theta1 xi}{\theta2 +xi})$ becomes $215$ ~ $\frac{\theta1*1}{\theta2+1}$


Solving it for $\theta1$ we get $\theta1 = 215/1.02 \iff \theta1 \approx 210.7$


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Estimating the MLE
out <- nlm(myLike,
  p = c(210.7,0.02,15), #plugging in the starting values
  hessian = T,
  iterlim = 10000,
  steptol = 1e-10)

# Reporting estimates
variableEstimates = out$estimate
out$estimate

```
## Question 1 e)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Invert the negated Hessian to obtain the Observed Information Matrix
OIM <- solve(out$hessian)

# The diagonal entries are the variances of beta0 and beta1 respectively so # obtain them
VarianceBeta <- diag(OIM)

# and then square root them to obtain standard errors
stand_error <- sqrt(VarianceBeta)

# reporting standard errors
stand_error


```
The formula to calculate a 99% confidence interval is: β ± 2.576 * SE(β)


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Estimating CIs
CIs <- cbind(variableEstimates - 2.576 * stand_error, variableEstimates + 2.576 * stand_error)

# Reporting the CIs
CIs


```


## Question 1 f)

H0 : θ2 = 0,08 vs. H1 : θ2 $\ne$ 0

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## Hypothesis thesis without using confidence interval

z_stat <- (variableEstimates[2] - 0.08)/stand_error[2]

# Print the test values
z_stat ## significance tests


```
So now we need to decide if this value of the z-statistic is extreme at the 10% significance level

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

### Note that equivalently we can look at the 95% quantile of N(0,1)
qnorm(0.95,0,1)

qnorm(0.05,0,1)

```
As we can see the value from the z-statistician test is considerably lower than -1,645, meaning that it is an extreme value and therefore rejecting the null hypothesis that $\theta2$ is 0,08. 

## Question 1 g)


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

# Plotting initial starting guess
xx <- seq(0, 1, len=200)

# Estimating mean relationship (mean mu = )
mu <- (out$estimate[1 ] * xx) /(out$estimate[2] + xx)

# Getting standard Deviation
standardDeviation <- out$estimate[3]


# Getting 95% interval from the quantiles of a Normal distribution
plot(nlmodel$x,nlmodel$y,pch=20, xlab = 'X variable', ylab = 'Y variable')
lines(xx, mu, col = 'yellow')
lines(xx, qnorm(0.025, mean = mu, sd = standardDeviation), col = 'blue', lty = 'dashed')
lines(xx, qnorm(0.975, mean = mu, sd = standardDeviation), col = 'blue', lty = 'dashed')

lines(xx, qnorm(0.5, mean = mu, sd = standardDeviation), col = 'red')



```


## Question 2

Model 1:

$Yi$ ~ $Pois(\lambda i)$

$log(\lambda i) = \beta0 + \beta1 xi$

Model 2:

$Yi$ ~ $N(\mu i,\sigma^2)$

$log(\mu i) = \gamma0 +\gamma1 xi$

## Question 2 a)

As we can from the graph and what we can determine from the nature of
the data represented in such graph the recorded number of AIDS cases is
a count variable and the counts are non-negative integers.

The first model, a Poisson distribution, would be a more appropriate
choice. The Poisson distribution is a discrete distribution that models
count data which respects the nature of the data

The second model, a Normal distribution, would not be the best fit since
its range is from ]-$\infty$,$\infty$[ and expects continuous values,
not respecting the nature of the data.

The log-link function in both models ensures that the predicted values
are always positive. //TODO redo this pls

## Question 2 b)

The Yi are independent so the likelihood is a product of the individual pdfs.

$L(\theta1,theta2,\sigma^2;y,x)$


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}
#Fitting in R
#model < −glm(< response >∼< covariates >,data = <data>, family = gaussian(link=’identity’))
#model <- glm(<response> ∼ <covariates>, data = <data>, family = poisson(link=’log’))


# Fitting model 2
# pois.model <- glm(ca ~ offset(logcells) + doseamt + doserate,
# data = dicentric,
# family = poisson(link="log"))

model2 = glm(cases ~ date, data = aids, family = poisson(link='log'))

# Summarise the model
summary(model2)


#Fitting model 1
model1 = glm(cases ~ date, data = aids, family = gaussian(link='identity'))

# Summarise the model
summary(model1)


```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

##plotting the models

##this is the original data to then plot the models on top of
ggplot(aids, aes(x= date, y = cases)) +
  geom_point()


#plot(model2)
#plot(model1)


confint(model1, level = 1-0.05)
confint(model2, level = 1-0.05)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

#As we are modelling an unbounded count we use Poisson distribution. The data increases exponentially so we use a log-link with a model linear in time.




```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, eval=FALSE}
## only testing here won't show up in the final report
load("C:/AppliedDataScienceAndStatistics/Applied-Data-Science-and-Statistics/Term2/StatisticalDataModeling/datasetsTopic2.RData")

# Fit the model
mod <- glm(y ~ t,
family = poisson,
data = aidsbelgium)

# Summarise the model
summary(mod)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}



```
