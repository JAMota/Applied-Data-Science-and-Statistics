---
title: "SpaceAndTime"
format: html
editor: visual
---


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, output=FALSE}
rm(list = ls())

library(class) # this has the knn function
library(ggplot2)
library(tidyverse)
library(tidyr)
library(DataExplorer)

netherlandsDF = read_csv("assessmentData/netherlands.csv")
californiaTempDF = read_csv("assessmentData/maxTempCalifornia.csv")
AMOCDF = read_csv("assessmentData/AMOCdata.csv")
californiaSpatialDataDF = read_csv("assessmentData/metadataCA.csv")

#install.packages('gstat')
library(geoR)
library(viridis)
library(sp)
library(gstat)
library(forecast)

library(dlm)


```

## Question 1 Spatial modelling dutch

### 1 a)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

ggplot(data = netherlandsDF) +
  geom_point(aes(x = longitude, y = latitude, size = precip, color = precip)) +
  scale_color_continuous(low = "blue", high = "red") +
  labs(title = "Precipitation in Netherlands Stations",
       x = "Longitude",
       y = "Latitude",
       size = "Precipitation",
       color = "Precipitation") +
  theme_minimal()

```
From what we can see from the data it does seem to be spatially correlated as we can that the Dutch provinces of north Holland, Friesland and Groningen has higher precipitation and as we go south the precipitation does decrease as we can see from the Dutch provinces of Zeeland, north Brabant and Limburg where precipitation is significantly lower than their northern counterparts. 

From this data, latitude seems to be the biggest factor in the variation of the precipitation as the longitude only suggests some slight variations in the data.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# Create geodata object
precip_geoR = as.geodata(netherlandsDF, coords.col = c("longitude", "latitude"), data.col = "precip")

```

### 1 b)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# set seed for reproducibility
set.seed(26041999)

# Select 3 random rows from the data frame
random_rows = netherlandsDF %>% sample_n(3)

# Add a new column with labels
random_rows$label = c("A", "B", "C")

# Print the randomly selected rows
random_rows

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# Remove the selected rows from the original dataset
netherlandsDF_filtered = netherlandsDF %>% anti_join(random_rows)

# Print the resulting dataframe
netherlandsDF_filtered

```

### 1 c)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# ##converting the dataframe into a geodata object
# 
# coordinates(netherlandsDF) = c("longitude", "latitude")
# proj4string(netherlandsDF) = CRS("+proj=longlat +datum=WGS84")
# 
# netherlandsDF_geodata = as.geodata(netherlandsDF)
# 
# 
# 
# sample_variogram = variogram(precip~1, netherlandsDF)
# 
# plot(sample_variogram, type = "b", main = "Variogram: Percipitation over Netherlands")


# # Calculate empirical variogram
precip_variog = variog(precip_geoR)

# Plot empirical variogram
plot(precip_variog)



```
From the plotted variogram we can see there a very clear need for a nugget as there is a non-zero value around zero distance.

The semi variance continuous to increase with distance, as such there is no clear trend to help us decide in a clear cut off to set as our maximum distance, as such it is better to compare multiple models with different maximum distances to determine the most optimal maximum distance.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE, echo=FALSE}

coordinates(netherlandsDF) = ~longitude+latitude
geodata = as.geodata(netherlandsDF, coords.col = 1:2, data.col = 4)

sample_variogram = variog(geodata)


```

### 1 d)

## TODO

Initial asusmptions:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# Create a grid for interpolation
grid = expand.grid(longitude = seq(min(netherlandsDF$longitude), max(netherlandsDF$longitude),
          length.out = 100),
          latitude = seq(min(netherlandsDF$latitude), max(netherlandsDF$latitude), length.out = 100))

coordinates(grid) = ~longitude + latitude
gridded(grid) = TRUE

# Perform ordinary Kriging
kriging_result = krige(precip ~ 1, netherlandsDF, grid, model = fitted_variogram)


```

1 h) he predicts using the grid but we have to change it to our own coordinate and the prior use previous results

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

## Question 2

### 2 a)

We fist start by making the appropriate changes in the data to average the data to quarterly means

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

AMOCDF$Date = as.Date(AMOCDF$Date, format = "%d/%m/%Y")

## I will now make a column with the quarter and year that I will use to create the averages per quarter
AMOCDF$YearQuarter = paste(AMOCDF$Year, AMOCDF$Quarter, sep = "-")

YearQuarterAverage = AMOCDF %>%
  group_by(YearQuarter) %>%
  summarise(AverageStrength = mean(Strength))



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, eval=FALSE }

## nevermind ts object does this

YearQuarterAverage$Year = str_extract(YearQuarterAverage$YearQuarter, "\\d{4}")

YearQuarterAverage$Quarter = str_extract(YearQuarterAverage$YearQuarter, "(?<=Q)\\d")

YearQuarterAverage$YearLabel = ifelse(YearQuarterAverage$Quarter == 1,
                                       YearQuarterAverage$Year,"")

# plot with year label
ggplot(YearQuarterAverage, aes(x = YearQuarter, y = AverageStrength)) +
  geom_point() +
  geom_smooth(method = "lm") +
  xlab("Year") +
  ylab("Average Strength") +
  ggtitle("Average Strength by Year") +
  scale_x_discrete(labels = function(x) ifelse(seq_along(x) %% 4 == 1, x, ""))

#write.csv(YearQuarterAverage, "data.csv", row.names=FALSE)



```

Now we will convert the average data to a time series object to be able to plot it

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}


tsAtlantic = ts(YearQuarterAverage, start = c(2010, 1), frequency = 4)

tsAtlantic = tsAtlantic[, "AverageStrength"]

plot.ts(tsAtlantic)


```
#### Trend analysis

From this graph we can see a yearly oscillation of Sverdrups. We can also identify that the peaks in Sverdrups are usually in the last quarter before the start of a new year and the valleys are on the second quarter of the year.

The data does seem stationary enough that if we were to differentiate we would start losing some of the structure.


### 2 b)

#### ACF

First we will start by checking the ACF(Autocorrelation Function) and PACF(Partial Autocorrelation Function) to check for if we have stationary data or not to help us decide between an ARMA or an ARIMA model. 


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

acf(tsAtlantic)


```

We can see that for ACF OF Average strength slowly decreases as lag increases to infinity with lag = 3 still being a significant values, meaning it is not a simple MA model as AR is clearly not quickly cut-off.

#### PACF

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

pacf(tsAtlantic)

```

The PACF seems to be cut-off at lag 0,5 indicating an AR model might be a best fit for our data to be a   but with some almost significant values after the cut it might be also appropriate to some non-zero q values to confirm our initial assumption

As such we will now proceed to fit multiple model firstly with the initial assumption that, then I will both use models with non-zero q and the model given by the auto.arima function to double check that the assumptions made by the previous analyses is correct.

<!-- The PACF of the average strength has a mix values above and bellow the relevance interval so although it does look like there is a gradual decrease as lag increases to infinity it is not very conclusive and it can also be a sharp cut-off instead. -->

<!-- Since both ACF and PACF seem to be slowly decaying and the the previous question plot does not seem to indicate any trend that needs to be differentiated, we will start by fitting multiple ARMA models with multiple choices of p and q values -->

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# it is always a good practice to try multiple values of p,d and q to see if we can do better
## we then obviously compare via the AIC of the models and their log likelihoods
## it is never enough to check those we also need to check the residuals

## order is p, d ,q 

##initial models under our assumptions

model100 = Arima(tsAtlantic, order = c(1,0,0))
model200 = Arima(tsAtlantic, order = c(2,0,0))
model300 = Arima(tsAtlantic, order = c(3,0,0))

## now I will add postive q values

model101 = Arima(tsAtlantic, order = c(1,0,1))
model102 = Arima(tsAtlantic, order = c(1,0,2))
model103 = Arima(tsAtlantic, order = c(1,0,3))

model201 = Arima(tsAtlantic, order = c(2,0,1))
model202 = Arima(tsAtlantic, order = c(2,0,2))
model203 = Arima(tsAtlantic, order = c(2,0,3))

model301 = Arima(tsAtlantic, order = c(3,0,1))
model302 = Arima(tsAtlantic, order = c(3,0,2))
model303 = Arima(tsAtlantic, order = c(3,0,3))



## lastly we will use auto.arima without seasonality to confirm our inital assumptions
modelAuto = auto.arima(tsAtlantic, max.d = 0, max.p = 5, max.q = 5, seasonal = FALSE)


```

#### best model selection

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }
model100
model200
model300

```
As we can see from these inital models ARIMA(2,0,0) is the model that has the best fit has we can see from its lower AIC score of 194,9.

Now we will check against the other models to check the validity of our assumptions.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

model101
model102
model103

model201
model202
model203

model301
model302
model303

```
In this initial analysis we have found models that do have a lower AIC lower log likelihood than our previous best model, however these model ma's standard error are to close the the ma values indicating that while we are getting a better fit we might be overfitting to our data.

As such this does confirm our initial assumption for the choice of a zero q value.

Now lastly we will check if the auto.arima function does comfirm our initial assumptions.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

modelAuto

```
The function does confirm our assumption that ARIMA(2,0,0) is indeed the best model.

We will now check the residuals to verify if any of ou previously selected model validates well or if it is simply the best of bad models.

## talk about the model being more easily explainability  becaues MA = 0


#### Best model residual validation

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

tsdiag(model200)

```
Initially from the standardised residuals plot we can identify some sort of sinusoidal pattern, this implies that there is a seasonal trend that is not being accounted for in our model and as such this trends needs to be accounted in future models to better explain and increase the prediction power of a new model.


#### Forecasting 

Now using the forecast function we will forecast the next 4 quarters of 2021 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

forecast(model200, 4)


```

But this data is better visualized in a graph to better understand if the predictions are sensible compared to our real data.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

predictedArimaDF = data.frame(forecast(model200, 4))  

predictedArimaDF$YearQuarter =  c("2021-Q1"	, "2021-Q2", "2021-Q3","2021-Q4"	)

# Combine real_data and pred_data into a single data frame
combined_data <- rbind(
  data.frame(Date = YearQuarterAverage$YearQuarter, Temperature = YearQuarterAverage$AverageStrength, Type = "Real"),
  data.frame(Date = predictedArimaDF$YearQuarter, Temperature = predictedArimaDF$Point.Forecast, Type = "Predicted")
)

predictedArimaDF$Temperature = predictedArimaDF$Point.Forecast

predictedArimaDF$Type = "Predicted"


# Create the ggplot
p <- ggplot(combined_data, aes(x = Date, y = Temperature, color = Type, group = 1)) +
  geom_line() +
  scale_color_manual(values = c("blue", "red"))

# Add the 95% confidence interval
p <- p + geom_ribbon(
  data = predictedArimaDF, aes(x = YearQuarter, ymin = Lo.95, ymax = Hi.95),
  fill = "red",
  alpha = 0.2
)

# Display the plot
print(p)




```
As we can see from the graph the ARIMA (2,0,0) seems to give us a sensible forecast for the 2021 quarter values, however as we can see the interval of the prediction accuracy our model is not too certain on the values most likely due to our model not accounting for the seasonal cycle of our data.

### 2 c)

#### Initial assumptions

From the previous exploratory analysis of the data we have established that the data did not need to be differentiated since it was constant, this translates to polynomial DLM component of order 2 that will use linear model to account for this type of changes in the data.

Furthermore, from the residual analysis we have inferred that there is an underlying seasonal trend present on the data, this seasonal trend will be represented by a seasonal component of frequency 4 to represent the 4 quarters per year.

#### model fitting

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

## linear model, order = 2, quadratic order = 3 , etc

## what we want is a linear model with a seasonal component so we add the 2 components together in a model

## things to try, another term like quadratic, or a arma component stacked on top of this

## Initial model with a linear polynomial and a seasonal component

buildFun = function(x) {
dlmModPoly(order = 2, dV = exp(x[1]), dW = c(0, exp(x[2]))) +
dlmModSeas(frequency = 4, dV = 0, dW = c(exp(x[3]), rep(0,2)))
}

linearDLM = dlmMLE(tsAtlantic, parm = c(0,0,0), build = buildFun)

linearDLM$par

fittedLinearDLM = buildFun(linearDLM$par)

V(fittedLinearDLM)

W(fittedLinearDLM)


```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## second model with a quadratic polynomial and a seasonal component

buildFunQuad = function(x) {
  dlmModPoly(order = 3, dV = exp(x[1]), dW = c(0, exp(x[2]), exp(x[3]))) +
  dlmModSeas(frequency = 4, dV = 0, dW = c(exp(x[4]), rep(0,2)))
}

quadraticDLM = dlmMLE(tsAtlantic, parm = c(0,0,0,0), build = buildFunQuad)

quadraticDLM$par

fittedQuadraticDLM = buildFunQuad(quadraticDLM$par)

V(fittedQuadraticDLM)

W(fittedQuadraticDLM)

```
## TODO include dlm with arima?

First to compare both models we will use AIC to see if the extra flexibility from the extra polynomial function is providing a better fit

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE, echo=FALSE}


# Filter the data using the quadratic DLM model
quadFilter <- dlmFilter(tsAtlantic, fittedQuadraticDLM)

# Calculate the log-likelihood of the filtered data
logLik <- sum(quadFilter$logLik)

# Calculate the number of parameters in the model
numParams <- length(quadraticDLM$par)

# Calculate the AIC
AIC <- 2 * numParams - 2 * logLik
AIC


# Filter the data using the quadratic DLM model
quadFilter <- dlmFilter(tsAtlantic, fittedLinearDLM)

# Calculate the log-likelihood of the filtered data
logLik <- sum(quadFilter$logLik)

# Calculate the number of parameters in the model
numParams <- length(linearDLM$par)

# Calculate the AIC
AIC <- 2 * numParams - 2 * logLik
AIC


```


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

amocPredict <- dlmFilter(tsAtlantic, mod = fittedLinearDLM)
summary(amocPredict)


```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

x <- cbind(tsAtlantic, dropFirst(amocPredict$a[,c(1,3)]))
x <- window(x, start = c(2010,1))
colnames(x) <- c("Gas", "Trend", "Seasonal")
plot(x, type = 'o', main = "Atlantic AMOC at 26,5N 2010-2020")

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

amocForecast = dlmForecast(amocPredict, nAhead = 4)
summary(amocForecast)

dim(amocForecast$a)

dim(amocForecast$f)


```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

sqrtR <- sapply(amocForecast$R, function(x) sqrt(x[1,1]))
pl <- amocForecast$a[,1] + qnorm(0.025, sd = sqrtR)
pu <- amocForecast$a[,1] + qnorm(0.975, sd = sqrtR)
x <- ts.union(window(tsAtlantic, start = c(2010, 1)),
    amocForecast$a[,1],
    amocForecast$f, pl, pu)
par(mar=c(4,4,2,2))
plot(x, plot.type = "single", type = 'o', pch = c(1, 20, 3, NA, NA),
    col = c("darkgrey", "brown", "brown", "blue", "blue"),
    ylab = "Log gas consumption")

legend("bottomright", legend = c("Observed","Forecast", "95% interval"),
    bty = 'n', pch = c(1, 20, NA), lty = 1,
    col = c("darkgrey", "brown", "blue"))

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

tsdiag(amocPredict)

```

## Question 3

### Question 3 a)

I will start with the time series analysis of the temperature in California

other approach see max temp in the entire state with 8 cities

## TODO WARNING
For a dataset of daily data with only 1 year of a cycle data available a daily frequency won't be a very good fit because we only have one observation per cycle, we need a hidden entry to capture the 12 months instead

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



californiaTempDF$Date = as.Date(as.character(californiaTempDF$Date), format = "%Y%m%d", origin = "1970-01-01")

## its better to just get a time series object for each city and plot each of those in the same plot

tsCaliforniaSanDiegoTemp = ts(californiaTempDF$`San Diego`, start = c(2012,1), frequency = 366)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

plot.ts(tsCaliforniaSanDiegoTemp)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```