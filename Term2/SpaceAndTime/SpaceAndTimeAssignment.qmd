---
title: "SpaceAndTime"
format: 
  pdf:
    toc: true
    toc-depth: 4
editor: visual
---


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, output=FALSE}
rm(list = ls())

library(class) # this has the knn function
library(ggplot2)
library(tidyverse)
library(tidyr)
library(DataExplorer)

netherlandsDF = read_csv("assessmentData/netherlands.csv")
californiaTempDF = read_csv("assessmentData/maxTempCalifornia.csv")
AMOCDF = read_csv("assessmentData/AMOCdata.csv")
californiaSpatialDataDF = read_csv("assessmentData/metadataCA.csv")

#install.packages('gstat')
library(geoR)
library(viridis)
library(sp)
library(gstat)
library(forecast)

library(dlm)


```

## Question 1 Spatial modelling Kingdom of the Netherlands

### 1 a)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

ggplot(data = netherlandsDF) +
  geom_point(aes(x = longitude, y = latitude, size = precip, color = precip)) +
  scale_color_continuous(low = "blue", high = "red") +
  labs(title = "Precipitation in Netherlands Stations",
       x = "Longitude",
       y = "Latitude",
       size = "Precipitation",
       color = "Precipitation") +
  theme_minimal()

```
From what we can see from the data it does seem to be spatially correlated as we can that the Dutch provinces of north Holland, Friesland and Groningen has higher precipitation and as we go south the precipitation does decrease as we can see from the Dutch provinces of Zeeland, north Brabant and Limburg where precipitation is significantly lower than their northern counterparts. 

From this data, latitude seems to be the biggest factor in the variation of the precipitation as the longitude only suggests some slight variations in the data.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# Create geodata object
precipitationNetherland_geoR = as.geodata(netherlandsDF, coords.col = c("longitude", "latitude"), data.col = "precip")

summary(precipitationNetherland_geoR)

```
<!-- https://notepub.io/notes/mathematics/statistics/descriptive-statistics/descriptive-statistics-moments-skewness-and-kurtosis/ -->
<!-- Central Tendency Measures in  Negatively, Zero, Positively Skewed Curve -->


As we can see from the numerical summary of the data the median is different from the mean, which indicates it is not a symmetric distribution of data points and is instead positively skewed since the mean is bigger than the median. 
As such there are more values on the left side of the distribution.

### 1 b)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# set seed for reproducibility
set.seed(26041999)

# Select 3 random rows from the data frame
randomRowsPrecipitation = netherlandsDF %>% sample_n(3)

# Add a new column with labels
randomRowsPrecipitation$label = c("A", "B", "C")

# Print the randomly selected rows
randomRowsPrecipitation

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# Remove the selected rows from the original dataset
netherlandsDF_filtered = netherlandsDF %>% anti_join(randomRowsPrecipitation)

# Print the resulting dataframe
netherlandsDF_filtered

```

### 1 c)


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }


# # Calculate empirical variogram
variogramPrecipitationNetherlands = variog(precipitationNetherland_geoR)

# Plot empirical variogram
plot(variogramPrecipitationNetherlands)

variogramPrecipitationNetherlands$n

```
From the plotted variogram we can see there a very clear need for a nugget as there is a non-zero value around zero distance, this values seems to be around  100 at the zero distance from how much is it decreasing.

The semi variance continuous to increase with distance till around the distance of 2 degrees distance wise, after this there is a decrease in variance that is not representative of the data as we are more and more uncertain the further we are from our known points, as such we will choose the distance of two as the cut off for the maximum distance.

<!-- , as such there is no clear trend to help us decide in a clear cut off to set as our maximum distance, as such it is better to compare multiple models with different maximum distances to determine the most optimal maximum distance. -->

we know change the maximum distance change and recut our previous variogram.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

variogramPrecipitationNetherlands = variog(precipitationNetherland_geoR, option='bin', max.dist = 2)

```


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE, echo=FALSE}

coordinates(netherlandsDF) = ~longitude+latitude
geodata = as.geodata(netherlandsDF, coords.col = 1:2, data.col = 4)

sample_variogram = variog(geodata)


```

### 1 d)


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), eval=FALSE, echo=FALSE}

# Create a grid for interpolation
grid = expand.grid(longitude = seq(min(netherlandsDF$longitude), max(netherlandsDF$longitude),
          length.out = 100),
          latitude = seq(min(netherlandsDF$latitude), max(netherlandsDF$latitude), length.out = 100))

coordinates(grid) = ~longitude + latitude
gridded(grid) = TRUE

# Perform ordinary Kriging
kriging_result = krige(precip ~ 1, netherlandsDF, grid, model = fitted_variogram)


```

## TODO look for better initial values.

Now that we have the variogram we will start by fitting a model to estimate the covariance via weightled least squares.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

#?variofit

krigingVariogramFitted <- variofit(variogramPrecipitationNetherlands)

krigingVariogramFitted

```
fitting this variogram we get the estimated values of $\sigma^2, \phi$ and $\tau^2$ also known as the nugget


1 h) he predicts using the grid but we have to change it to our own coordinate and the prior use previous results

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

## Question 2

### 2 a)

We fist start by making the appropriate changes in the data to average the data to quarterly means

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

AMOCDF$Date = as.Date(AMOCDF$Date, format = "%d/%m/%Y")

## I will now make a column with the quarter and year that I will use to create the averages per quarter
AMOCDF$YearQuarter = paste(AMOCDF$Year, AMOCDF$Quarter, sep = "-")

YearQuarterAverage = AMOCDF %>%
  group_by(YearQuarter) %>%
  summarise(AverageStrength = mean(Strength))



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72), echo=FALSE, eval=FALSE }

## nevermind ts object does this

YearQuarterAverage$Year = str_extract(YearQuarterAverage$YearQuarter, "\\d{4}")

YearQuarterAverage$Quarter = str_extract(YearQuarterAverage$YearQuarter, "(?<=Q)\\d")

YearQuarterAverage$YearLabel = ifelse(YearQuarterAverage$Quarter == 1,
                                       YearQuarterAverage$Year,"")

# plot with year label
ggplot(YearQuarterAverage, aes(x = YearQuarter, y = AverageStrength)) +
  geom_point() +
  geom_smooth(method = "lm") +
  xlab("Year") +
  ylab("Average Strength") +
  ggtitle("Average Strength by Year") +
  scale_x_discrete(labels = function(x) ifelse(seq_along(x) %% 4 == 1, x, ""))

#write.csv(YearQuarterAverage, "data.csv", row.names=FALSE)



```

Now we will convert the average data to a time series object to be able to plot it

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}


tsAtlantic = ts(YearQuarterAverage, start = c(2010, 1), frequency = 4)

tsAtlantic = tsAtlantic[, "AverageStrength"]

plot.ts(tsAtlantic)


```
#### Trend analysis

From this graph we can see a yearly oscillation of Sverdrups. We can also identify that the peaks in Sverdrups are usually in the last quarter before the start of a new year and the valleys are on the second quarter of the year.

The data does seem stationary enough that if we were to differentiate we would start losing some of the structure.


### 2 b)

#### ACF

First we will start by checking the ACF(Autocorrelation Function) and PACF(Partial Autocorrelation Function) to check for if we have stationary data or not to help us decide between an ARMA or an ARIMA model. 


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

acf(tsAtlantic)


```

We can see that for ACF OF Average strength slowly decreases as lag increases to infinity with lag = 3 still being a significant values, meaning it is not a simple MA model as AR is clearly not quickly cut-off.

#### PACF

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

pacf(tsAtlantic)

```

The PACF seems to be cut-off at lag 0,5 indicating an AR model might be a best fit for our data to be a   but with some almost significant values after the cut it might be also appropriate to some non-zero q values to confirm our initial assumption

As such we will now proceed to fit multiple model firstly with the initial assumption that, then I will both use models with non-zero q and the model given by the auto.arima function to double check that the assumptions made by the previous analyses is correct.

<!-- The PACF of the average strength has a mix values above and bellow the relevance interval so although it does look like there is a gradual decrease as lag increases to infinity it is not very conclusive and it can also be a sharp cut-off instead. -->

<!-- Since both ACF and PACF seem to be slowly decaying and the the previous question plot does not seem to indicate any trend that needs to be differentiated, we will start by fitting multiple ARMA models with multiple choices of p and q values -->

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# it is always a good practice to try multiple values of p,d and q to see if we can do better
## we then obviously compare via the AIC of the models and their log likelihoods
## it is never enough to check those we also need to check the residuals

## order is p, d ,q 

##initial models under our assumptions

model100 = Arima(tsAtlantic, order = c(1,0,0))
model200 = Arima(tsAtlantic, order = c(2,0,0))
model300 = Arima(tsAtlantic, order = c(3,0,0))

## now I will add postive q values

model101 = Arima(tsAtlantic, order = c(1,0,1))
model102 = Arima(tsAtlantic, order = c(1,0,2))
model103 = Arima(tsAtlantic, order = c(1,0,3))

model201 = Arima(tsAtlantic, order = c(2,0,1))
model202 = Arima(tsAtlantic, order = c(2,0,2))
model203 = Arima(tsAtlantic, order = c(2,0,3))

model301 = Arima(tsAtlantic, order = c(3,0,1))
model302 = Arima(tsAtlantic, order = c(3,0,2))
model303 = Arima(tsAtlantic, order = c(3,0,3))



## lastly we will use auto.arima without seasonality to confirm our inital assumptions
modelAuto = auto.arima(tsAtlantic, max.d = 0, max.p = 5, max.q = 5, seasonal = FALSE)


```

#### best model selection

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }
model100
model200
model300

```
As we can see from these inital models ARIMA(2,0,0) is the model that has the best fit has we can see from its lower AIC score of 194,9.

Now we will check against the other models to check the validity of our assumptions.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

model101
model102
model103

model201
model202
model203

model301
model302
model303

```
In this initial analysis we have found models that do have a lower AIC lower log likelihood than our previous best model, however these model ma's standard error are to close the the ma values indicating that while we are getting a better fit we might be overfitting to our data.

As such this does confirm our initial assumption for the choice of a zero q value.

Now lastly we will check if the auto.arima function does comfirm our initial assumptions.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

modelAuto

```
The function does confirm our assumption that ARIMA(2,0,0) is indeed the best model.

We will now check the residuals to verify if any of ou previously selected model validates well or if it is simply the best of bad models.

## talk about the model being more easily explainability  becaues MA = 0


#### Best model residual validation

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# Set smaller margins
par(mar = c(4, 4, 2, 2))

tsdiag(model200)

# Reset margins
par(mar = c(5, 4, 4, 2) + 0.1)



```
Initially from the standardised residuals plot we can identify some sort of sinusoidal pattern, this implies that there is a seasonal trend that is not being accounted for in our model and as such this trends needs to be accounted in future models to better explain and increase the prediction power of a new model.


#### Forecasting 

Now using the forecast function we will forecast the next 4 quarters of 2021 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

forecast(model200, 4)


```

But this data is better visualized in a graph to better understand if the predictions are sensible compared to our real data.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

predictedArimaDF = data.frame(forecast(model200, 4))  

predictedArimaDF$YearQuarter =  c("2021-Q1"	, "2021-Q2", "2021-Q3","2021-Q4"	)

# Combine real_data and pred_data into a single data frame
combinedDataframeAMOC <- rbind(
  data.frame(Date = YearQuarterAverage$YearQuarter, Temperature = YearQuarterAverage$AverageStrength, Type = "Real"),
  data.frame(Date = predictedArimaDF$YearQuarter, Temperature = predictedArimaDF$Point.Forecast, Type = "Predicted")
)

predictedArimaDF$Temperature = predictedArimaDF$Point.Forecast

predictedArimaDF$Type = "Predicted"


# Create the ggplot
plotARIMA <- ggplot(combinedDataframeAMOC, aes(x = Date, y = Temperature, color = Type, group = 1)) +
  geom_line() +
  scale_color_manual(values = c("blue", "red"))

# Add the 95% confidence interval
plotARIMA <- plotARIMA + geom_ribbon(
  data = predictedArimaDF, aes(x = YearQuarter, ymin = Lo.95, ymax = Hi.95),
  fill = "red",
  alpha = 0.2
)

# Adjust the x-axis labels
plotARIMA <- plotARIMA + scale_x_discrete(
  breaks = combinedDataframeAMOC$Date[c(TRUE, rep(FALSE, 3))],
  labels = combinedDataframeAMOC$Date[c(TRUE, rep(FALSE, 3))]
)

plotARIMA <- plotARIMA + theme(
  axis.text.x = element_text(angle = 90, vjust = 0.5, size = 8)
)

# Display the plot
print(plotARIMA)




```
As we can see from the graph the ARIMA (2,0,0) seems to give us a sensible forecast for the 2021 quarter values, however as we can see the interval of the prediction accuracy our model is not too certain on the values most likely due to our model not accounting for the seasonal cycle of our data.

### 2 c)

#### Initial assumptions

From the previous exploratory analysis of the data we have established that the data did not need to be differentiated since it was constant, this translates to polynomial DLM component of order 2 that will use linear model to account for this type of changes in the data.

Furthermore, from the residual analysis we have inferred that there is an underlying seasonal trend present on the data, this seasonal trend will be represented by a seasonal component of frequency 4 to represent the 4 quarters per year.

#### model fitting

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

## linear model, order = 2, quadratic order = 3 , etc

## what we want is a linear model with a seasonal component so we add the 2 components together in a model

## things to try, another term like quadratic, or a arma component stacked on top of this

## Initial model with a linear polynomial and a seasonal component

buildFun = function(x) {
dlmModPoly(order = 2, dV = exp(x[1]), dW = c(0, exp(x[2]))) +
dlmModSeas(frequency = 4, dV = 0, dW = c(exp(x[3]), rep(0,2)))
}

linearDLM = dlmMLE(tsAtlantic, parm = c(0,0,0), build = buildFun)

linearDLM$par

fittedLinearDLM = buildFun(linearDLM$par)

V(fittedLinearDLM)

W(fittedLinearDLM)



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72)}

## second model with a quadratic polynomial and a seasonal component

buildFunQuad = function(x) {
  dlmModPoly(order = 3, dV = exp(x[1]), dW = c(0, exp(x[2]), exp(x[3]))) +
  dlmModSeas(frequency = 4, dV = 0, dW = c(exp(x[4]), rep(0,2)))
}

quadraticDLM = dlmMLE(tsAtlantic, parm = c(0,0,0,0), build = buildFunQuad)

quadraticDLM$par

fittedQuadraticDLM = buildFunQuad(quadraticDLM$par)

V(fittedQuadraticDLM)

W(fittedQuadraticDLM)

```
## TODO include dlm with arima?

Now we will compare both models through their log likelihood using the dlmLL function and see if the extra flexibility from the extra polynomial function is providing a better fit


```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

dlmLL(tsAtlantic, fittedLinearDLM)

dlmLL(tsAtlantic, fittedQuadraticDLM)

```
As we can see the dlm model using only a linear polynomial has a lower log likelihood than the model with an extra quadratic term, meaning this extra flexibility does not contribute to a better model fit and as such we will use the linear fitted model to do our forecasting.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

amocPredict <- dlmFilter(tsAtlantic, mod = fittedLinearDLM)
summary(amocPredict)


```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

x <- cbind(tsAtlantic, dropFirst(amocPredict$a[,c(1,3)]))
x <- window(x, start = c(2010,1))
colnames(x) <- c("Gas", "Trend", "Seasonal")
plot(x, type = 'o', main = "Atlantic AMOC at 26,5N 2010-2020")

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

amocForecast = dlmForecast(amocPredict, nAhead = 4)
summary(amocForecast)

dim(amocForecast$a)

dim(amocForecast$f)


```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

sqrtR <- sapply(amocForecast$R, function(x) sqrt(x[1,1]))
pl <- amocForecast$a[,1] + qnorm(0.025, sd = sqrtR)
pu <- amocForecast$a[,1] + qnorm(0.975, sd = sqrtR)
x <- ts.union(window(tsAtlantic, start = c(2010, 1)),
    amocForecast$a[,1],
    amocForecast$f, pl, pu)
par(mar=c(4,4,2,2))
plot(x, plot.type = "single", type = 'o', pch = c(1, 20, 3, NA, NA),
    col = c("darkgrey", "brown", "brown", "blue", "blue"),
    ylab = "Log gas consumption")

legend("bottomright", legend = c("Observed","Forecast", "95% interval"),
    bty = 'n', pch = c(1, 20, NA), lty = 1,
    col = c("darkgrey", "brown", "blue"))

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# Set smaller margins
par(mar = c(4, 4, 2, 2))

tsdiag(amocPredict)

# Reset margins
par(mar = c(5, 4, 4, 2) + 0.1)




```
### 2 d)

Again comparing the forecast values and their respective prediction intervals as we can see from the graphs bellow the dlm model has smaller prediction intervals, most likely due to being able to explain the underlying seasonal trend reducing therefore the uncertainty in comparison the ARIMA model.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

print(plotARIMA)

sqaretRoot <- sapply(amocForecast$R, function(x) sqrt(x[1,1]))
predictionLow <- amocForecast$a[,1] + qnorm(0.025, sd = sqaretRoot) ## Low
predictionUpper <- amocForecast$a[,1] + qnorm(0.975, sd = sqaretRoot) ## Upper 
x <- ts.union(window(tsAtlantic, start = c(2010, 1)),
    amocForecast$a[,1],
    amocForecast$f, predictionLow, predictionUpper)
par(mar=c(4,4,2,2))
plot(x, plot.type = "single", type = 'o', pch = c(1, 20, 3, NA, NA),
    col = c("darkgrey", "brown", "brown", "blue", "blue"),
    ylab = "Log gas consumption")

legend("bottomright", legend = c("Observed","Forecast", "95% interval"),
    bty = 'n', pch = c(1, 20, NA), lty = 1,
    col = c("darkgrey", "brown", "blue"))




```

### 2 e)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

# AMOCDFMonthly <- AMOCDF %>%
#   mutate(YearMonth = paste0(year(Date), "-", month(Date, label = TRUE, abbr = FALSE)))


```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

## Question 3

### Question 3 a)

I will start with the time series analysis of the temperature in California

other approach see max temp in the entire state with 8 cities

## TODO WARNING
For a dataset of daily data with only 1 year of a cycle data available a daily frequency won't be a very good fit because we only have one observation per cycle, we need a hidden entry to capture the 12 months instead

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



californiaTempDF$Date = as.Date(as.character(californiaTempDF$Date), format = "%Y%m%d", origin = "1970-01-01")

## its better to just get a time series object for each city and plot each of those in the same plot

tsCaliforniaSanDiegoTemp = ts(californiaTempDF$`San Diego`, start = c(2012,1), frequency = 366)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

plot.ts(tsCaliforniaSanDiegoTemp)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

californiaLongTempDF = pivot_longer(californiaTempDF, 
                               cols = -Date, 
                               names_to = "Location", 
                               values_to = "Temperature")

spatialTemperatureCaliforniaDF = merge(californiaLongTempDF, californiaSpatialDataDF)


ggplot(data = spatialTemperatureCaliforniaDF) +
  geom_point(aes(x = Lat, y = Long, color = Temperature, size = Temperature)) +
  scale_color_continuous(low = "blue", high = "red") +
  labs(title = "Precipitation in Netherlands Stations",
       x = "Longitude",
       y = "Latitude",
       color = "Precipitation") +
  theme_minimal()

```

### 3 b)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



geoDataCalifornia= as.geodata( spatialTemperatureCaliforniaDF, coords.col = 4:5, data.col = "Temperature", covar.col = "Elev")

variogramCalifornia= variog(geoDataCalifornia)


```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }

plot(variogramCalifornia)

```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=72) }



```
