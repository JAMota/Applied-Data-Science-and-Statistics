---
title: "DataQueueAnalysis"
format: html
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a
finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that
includes both content and the output of embedded code. You can embed
code like this:

```{r, echo=FALSE, output=FALSE}
library(stats4)
#install.packages("pracma")
library(RSQLite)
library(ssh)
library(tidyverse)
library(lubridate)
library(pracma)

#install.packages("maxLik")
library(maxLik)

```

```{r, echo=FALSE, output=FALSE}}
##session
databaseServerIP = "20.13.124.83"
keyFile = "C:/Users/AndreMota/Downloads/DataBaseServerWE_key.pem"
hostDatabaseServer = "sqluser@20.13.124.83:/opt/sqlite3/heart.db"

```

Connect to the consumer server to access the data in the database

```{r}

# Define the source and destination paths
source_path <- hostDatabaseServer
destination_path <- "C:/AppliedDataScienceAndStatistics/Applied-Data-Science-and-Statistics/Term3/heartData/"

# Execute the scp command
command <- paste("scp -i", keyFile, source_path, destination_path) ## add -f to force a new file
system(command)



```
Open the file

```{r}

# Connect to the database
heartData <- dbConnect(SQLite(), dbname = "heartData/heart.db")


# Fetch data, casting timestamps as TEXT preserving precision
heartResult <- dbGetQuery(heartData, 
"SELECT id, heart_rate, chest_volume, blood_oxygen_concentration, 
CAST(producer_entry_timestamp AS TEXT) AS producer_entry_timestamp,
CAST(producer_sent_timestamp AS TEXT) AS producer_sent_timestamp,
CAST(kafka_entry_timestamp AS TEXT) AS kafka_entry_timestamp,
CAST(consumer_received_timestamp AS TEXT) AS consumer_received_timestamp,
CAST(consumer_finished_timestamp AS TEXT) AS consumer_finished_timestamp
FROM heart_messages")

# Convert timestamp columns to numeric and to milliseconds
heartResult$producer_entry_timestamp <- as.numeric(heartResult$producer_entry_timestamp) * 1000
heartResult$producer_sent_timestamp <- as.numeric(heartResult$producer_sent_timestamp) * 1000
heartResult$kafka_entry_timestamp <- as.numeric(heartResult$kafka_entry_timestamp) 
heartResult$consumer_received_timestamp <- as.numeric(heartResult$consumer_received_timestamp) * 1000
heartResult$consumer_finished_timestamp <- as.numeric(heartResult$consumer_finished_timestamp) * 1000


# Print full values to check for correct conversion
options(digits = 20) # Increase number of significant digits
print(head(heartResult$producer_entry_timestamp))


# Disconnect from the database
dbDisconnect(heartData)


```


Timestamps wrangling 

```{r}


## calculate execution time 
heartResult$producer_execution_time <- heartResult$producer_sent_timestamp - heartResult$producer_entry_timestamp
heartResult$consumer_execution_time <- heartResult$consumer_finished_timestamp - heartResult$consumer_received_timestamp

## calculate travel time 

heartResult$producer_travel_time <- heartResult$kafka_entry_timestamp - heartResult$producer_sent_timestamp


heartResult$producer_execution_time 
heartResult$consumer_execution_time

heartResult$producer_travel_time

heartResult$queue_service_time = 
  heartResult$consumer_finished_timestamp - heartResult$producer_entry_timestamp

```


```{r}

## use the broker travel times to calculate broker execution

brokerTravelTime = read.csv("heartData/averageTime.csv", header = FALSE, colClasses = c("character"))
colnames(brokerTravelTime) = "kafka_travel_time"

brokerTravelTime$kafka_travel_time = as.numeric(brokerTravelTime$kafka_travel_time) 

brokerTravelTime$kafka_travel_time = brokerTravelTime$kafka_travel_time /1000

n <- nrow(heartResult)

# Select the first n rows from brokerTravelTime
selectedBrokerTravelTime <- data.frame(brokerTravelTime[1:n, ])
colnames(selectedBrokerTravelTime) = "kafka_travel_time"

heartResult <- cbind(heartResult, selectedBrokerTravelTime)


heartResult$kafka_execution_time = heartResult$consumer_received_timestamp - heartResult$kafka_entry_timestamp  - heartResult$kafka_travel_time



```


#Exploratory data analysis


```{r}

heartDataExploration <- heartResult[, 2:4]

summary(heartDataExploration)


ggplot(heartDataExploration, aes(x = heartDataExploration$heart_rate)) +
  geom_histogram(binwidth = 1, fill = "red", color = "black") +
  labs(x = "Heart Rate", y = "Frequency") +
  xlim(40, 120) 


ggplot(heartDataExploration, aes(x = heartDataExploration$chest_volume)) +
  geom_histogram(binwidth = 1, fill = "green", color = "blue") +
  labs(x = "Chest Volume", y = "Frequency") 


ggplot(heartDataExploration, aes(x = heartDataExploration$blood_oxygen_concentration)) +
  geom_histogram(binwidth = 1, fill = "orange", color = "red") +
  labs(x = "Blood Oxygen Concentration", y = "Frequency") +
   xlim(0, 9000)




```

```{r}



```

## classical approach MLE

#Calculate average arrival rate and service time

```{r}

inter_arrival_times = diff(heartResult$producer_entry_timestamp) 

arrival_rate_lambda <- 1/mean(inter_arrival_times/1000) #convert mili to seconds

service_time_mu = 1 / mean(heartResult$queue_service_time)

traffic_intensity_rho = arrival_rate_lambda / service_time_mu

arrival_rate_lambda
service_time_mu
traffic_intensity_rho

summary(heartResult$consumer_execution_time)

```


## all data estimation
```{r}

logLik <- function(rho, x) {
  n <- length(x)
  return(-n*rho + sum(x)*log(rho) - sum(log(factorial(x))))
}


inter_arrival_times = inter_arrival_times/1000

# Perform the MLE
start <- c(traffic_intensity_rho=arrival_rate_lambda/service_time_mu)
  # initial value for rho
result <- maxLik(logLik, start=start, x=inter_arrival_times)

# Print the result
print(summary(result))



```


## filtered outlier data
```{r}

##filtering the data

selected_row <- heartResult$consumer_execution_time  # Replace with the actual column name

# Calculate the 99.5% quantile
quantile_995 <- quantile(selected_row, 0.995)

# Filter out values above the 99.5% quantile
filtered_data <- selected_row[selected_row <= quantile_995]

#summary(filtered_data)
#summary(heartResult$consumer_execution_time)
filtered_indices <- selected_row <= quantile_975
heartResultFiltered <- heartResult[filtered_indices, ]


## MLE


logLik <- function(rho, x) {
  n <- length(x)
  return(-n*rho + sum(x)*log(rho) - sum(log(factorial(x))))
}


inter_arrival_times_filtered = diff(heartResultFiltered$producer_entry_timestamp/1000)

lamda_filtered = 1/mean(inter_arrival_times_filtered)

service_time_mu_filtered = 1/mean(heartResultFiltered$queue_service_time)

traffic_intensity_rho_filtered=lamda_filtered/service_time_mu_filtered

arrival_times_filtered = 1/inter_arrival_times_filtered

# Perform the MLE
start_filtered <- c(traffic_intensity_rho_filtered=lamda_filtered/service_time_mu_filtered)


  # initial value for rho
result_filtered <- maxLik(logLik, start=start_filtered, x=arrival_times_filtered)

# Print the result
print(summary(result_filtered))



```


## what-if analysis 0,5
```{r}

heartResultPerformanceImproved = heartResult




## MLE


logLike <- function(rho, x) {
  n <- length(x)
  return(-n*rho + sum(x)*log(rho) - sum(log(factorial(x))))
}


inter_arrival_times_performance = diff(heartResultPerformanceImproved$producer_entry_timestamp/1000)

##Improving performance
inter_arrival_times_performance = inter_arrival_times_performance *2

arrival_times_performance = 1/mean(inter_arrival_times_performance) #lambda

arrival_times_performance2 = 1/inter_arrival_times_performance

service_time_mu_performance = 1/mean(heartResultPerformanceImproved$queue_service_time)

traffic_intensity_rho_performance=arrival_times_performance/service_time_mu_performance

# Perform the MLE
start_performance <- c(traffic_intensity_rho_performance=arrival_times_performance/service_time_mu_performance)
  # initial value for rho
result_performance <- maxLik(logLike, start=start_performance, x=arrival_times_performance2)

# Print the result
print(summary(result_performance))



```



## producer analysis


```{r}

## it is the same, because they both start at the same point
lambdaProducer = arrival_rate_lambda 

#muProducer = 1/ mean(heartResult$producer_execution_time)

muProducer = 1/0.02 #deterministic

rhoProducer = lambdaProducer / muProducer



```

## MLE producer
```{r}

logLike <- function(rho, x) {
  n <- length(x)
  return(-n*rho + sum(x)*log(rho) - sum(log(factorial(x))))
}


inter_arrival_times_filtered = diff(heartResultFiltered$producer_entry_timestamp/1000)

lamda_filtered = 1/mean(inter_arrival_times_filtered)

service_time_mu_filtered = 1/mean(heartResultFiltered$queue_service_time)

traffic_intensity_rho_filtered=lamda_filtered/service_time_mu_filtered

arrival_times_filtered = 1/inter_arrival_times_filtered

# Perform the MLE
start_producer <- c(rhoProducer = lambdaProducer / muProducer)
  # initial value for rho
result_filtered <- maxLik(logLik, start=start_producer, x=arrival_times_filtered)

# Print the result
print(summary(result_filtered))




```


```{r}



```


```{r}



```


```{r}


```


```{r}



```

## insights

```{r}


ggplot(heartResult, aes(x = heartResult$queue_service_time)) +
  geom_histogram(binwidth = 0.5, fill = "red", color = "black") +
  labs(x = "Queue total service time", y = "Frequency") 
#+  xlim(0, 40) +

ggplot(heartResult, aes(x = heartResult$queue_service_time)) +
  geom_histogram(binwidth = 0.5, fill = "red", color = "black") +
  labs(x = "Queue total service time", y = "Frequency") +
  xlim(0, 40) 



```

```{r}



```

```{r}




```


```{r}

# Create a scatter plot with the regression line
ggplot(heartResult, aes(x = id, y = queue_service_time)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(x = "ID", y = "Queue Service Time") +
  ylim(0,30) +
  ggtitle("Linear Regression: Queue Service Time")


```

